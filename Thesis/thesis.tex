\documentclass{amsart}
\usepackage{amsmath,amssymb}
\usepackage{seraf}
\usepackage{tikz}
\usepackage{todonotes}\presetkeys{todonotes}{color=blue!20}{}
\usepackage{bbm}
\usepackage{enumerate}

\title{A Formally Verified Proof of the Central Limit Theorem}
\author{Luke Serafin}
%\date{\today}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem*{conjecture*}{Conjecture}
\newtheorem*{theorem*}{Theorem}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem{example}[theorem]{Example}
\newtheorem*{claim}{Claim}

\newcommand{\bldset}[2]{\{{#1}\mid{#2}\}}
\newcommand{\bldseq}[2]{\langle{#1}\mid{#2}\rangle}
\renewcommand{\E}{\mathbb E}
\newcommand\indep{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newcommand\sinc{\mathop{\text{sinc}}\nolimits}
\newcommand\Si{\text{Si}}
\newcommand\floor[1]{\lfloor {#1} \rfloor}

\begin{document}

\begin{abstract}
We present a formalization of the central limit theorem in the interactive proof assisstant Isabelle.
\end{abstract}

\maketitle

\section{Introduction}

Consider a toss of a fair coin. If we treat a result of tails as having value zero and a result of heads as having a result of one, we may treat the coin toss as a random variable, say $X$. Thus $X$ is supported on $\{0,1\}$, and $\P[X = 0] = \P[X = 1] = \frac{1}{2}$. Hence the expected value of $X$ is

\[ \E[X] = 0 \cdot \P[X = 0] + 1 \cdot \P[X = 1] = \frac{1}{2}. \]

Now suppose we toss the coin repeatedly, thus generating an infinite sequence $\bldseq{X_n}{n \in \N}$ of random variables which are pairwise independent and have the same distribution as $X$. By the strong law of large numbers, the mean $\overline X_n = \frac{1}{n} \sum_{i \le n} X_i$ converges almost surely to $\E[X] = \frac{1}{2}$. But clearly after a finite number of trials there is a nonzero probability that the value of $\overline X_n$ will differe from $\E[X]$. In fact, for $n$ odd the probability of deviation is $1$, because in this case it is impossible for $\frac{1}{n} \sum{i \le n} X_i$ to have the value $\frac{1}{2}$ at any element of the sample space. Nevertheless $|\overline X_n - \E[X]|$ must converge to zero, and so the probability of large deviations of the mean $\overline X_n$ from the expected value $\E[X]$ is small. Exactly how small is made precise by De Moivre's central limit theorem.

In 1733 De Moivre privately circulated a proof which, in modern terminology, shows that $n^{-1/2} \overline X_n$ converges to a normal distribution. This material was later published in the 1738 second edition of his book {\em The Doctrine of Chances,} the first edition of which was first published in 1712 and is widely regarded as the first textbook on probability theory. De Moivre also considered the case of what we might call a biased coin (an event which has value one with probability $p$ and zero with probability $1-p$, for some $p \in [0,1]$), and realized that his convergence result continues to hold in this case.

De Moivre's result was generalized by Laplace in the period between about 1776 and 1812 to sums of random variables with various other distributions. For example, in 1776 Laplace proved that $n^{-1/2} \overline X_n$ converges to a normal distribution in the case where the $X_n$'s are uniformly distributed. The particular problem Laplace considered in that paper was finding the distribution of the average inclination of a random sample of comets, the distribution for a single comet being assumed uniform between $0^\circ$ and $90^\circ$. Over the next three decades Laplace developed the conceptual and analytical tools to extend this convergence theorem to sums of independent identically distributed random variables with ever more general distributions, and this work culminated in his treatise {\em Th\'eorie analytique des probabilit\'es}. This included the development of the method of characteristic functions to study the convergence of sums of random variables, a move which firmly established the usefulness of analytic methods in probability theory (in particular Fourier analysis, the characteristic function of a random variable being exactly the Fourier transform of that variable).

Laplace's theorem, which later became known as the central limit theorem (a designation due to P\'olya and stemming from its importance both in the theory and applications of probability), states in modern terms that the normalized sum of a sequence of independent and identically distributed random variables converges to a normal distribution, provided the distribution of the random variables being summed guarantees they have a high probability of being small. All of this imprecise language will be made precise later on. In the work of Laplace all the main ingredients of the proof of the central limit theorem are present, though of course the theorem was refined and extended as probability underwent the radical changes necessitated by its move to measure-theoretic foundations in the first half of the twentieth century.

Gauss was one of the first to recognize the importance of the normal distribution to the estimation of measurement errors, and it is notable that the usefulness of the normal distribution in this context is largely a consequence of the central limit theorem, for errors occurring in practice are frequently the result of many independent factors which sum to an overall error in a way which can be regarded as approximated by a sum of independent and identically distributed random variables. The normal distribution also arose with surprising frequency in a wide variety of empirical contexts: from the heights of men and women to the velocities of molecules in a gas. This gave the central limit theorem the character of a natural law, as seen in the following poetic quote from Sir Francis Galton in 1889:
\begin{quote}
 I know of scarcely anything so apt to impress the imagination as the wonderful form of cosmic order expressed by the ``Law of Frequency of Error.'' The law would have been personified by the Greeks and deified, if they had known of it. It reigns with serenity and in complete self-effacement, amidst the wildest confusion. The huger the mob, and the greater the apparent anarchy, the more perfect is its sway. It is the supreme law of Unreason. Whenever a large sample of chaotic elements are taken in hand and marshaled in the order of their magnitude, an unsuspected and most beautiful form of regularity proves to have been latent all along.
\end{quote}

Standards of rigour have evloved a great deal over the course of the history of the central limit theorem, and around the turn of the twentieth century a completely precise notion of proof, developed by Frege, Russell, and many others, finally became available to mathematicians. Actually writing proofs which conform to the precise requirements of this notion did not become the new norm of mathematical practice, however, largely because it is impractical for human mathematicians to work at that level of formal detail. The burden of writing an entirely precise proof in first-order logic (say) simply does not offer sufficient gain for a human mathematician to undertake it. However, advances in automated computing technology around the middle of the twentieth century quickly progressed to the point where a computer could be programmed to take on the cumbersome burden of verifying all the details of a proof which a human outlined at a high level. This is the domain of interactive theorem proving.

%TODO: Insert a little background on interactive theorem proving and mention Professor Avigad's work on the prime number theorem.

A theorem which both played a fundamental role in the development of modern probability theory and has far-reaching applications seemed to us a perfect candidate for formalization, especially because the measure-theoretic libraries of Isabelle are still under active development and we saw and opportunity to contribute to them by formalizing the characteristic function arguments used to prove the CLT. The formalization was completed between 2011 and 2013, and improvements to the proof scripts are ongoing.

\section{Motivation}

The author was seeking a research project during his first year at Carnegie Mellon University, and discussed this with Professor Jeremy Avigad, who was leading a seminar on the history of mathematics at the time. Avigad suggested a mathematical formalization project, and the author began learning the Isabelle interactive proof assistant. Avigad suggested that improving the Isabelle integration libraries would be a useful goal, and suggested the author choose a particular result to focus on, as this would naturally result in improvements to the library. The author selected the central limit theorem, thinking that would be a good result to start with as it employs general integration theory in a nontrivial way. He was initially under the impression that would take the first part of the summer, but it soon became clear that this project was much more ambitious than initially anticipated.

The formalization of the central limit theorem was begun in the summer of 2012 using an LSEC grant from the department of philosophy, and continued the following summer with an REU grant obtained by the first author. \todo{Give grant number.} Johannes H\"olzl, while a doctoral student at Technische Universit\"at M\"unchen, had previously formalized a significant amount of measure theory in Isabelle \cite{hoelzl-measure}, and helped us significantly with the formalization effort during the authors' week-long visit to his university in the summer of 2013. The formalized proof was finally finished during the 2013-2014 academic year, and Johannes presented a preliminary report at the 2014 summer of logic.\todo{Add citation.} The second author returned to the project in 2015 in order to complete his master's thesis.

As suggested above, the main motive of this project was to improve the Isabelle libraries, in particular those relating to integration, by formalizing a new result. As its name suggests, the central limit theorem is of fundamental importance to probability theory and statistics, and the depth of its proof via characteristic functions (called Fourier transforms in other contexts) would put the analysis libraries of Isabelle to a rigorous test. The fact that our effort to formalize the central limit theorem succeeded in a few months of dedicated formalization effort (interspersed among longer stretches of slower progress) testifies to the maturity of the analysis and measure theory libraries in Isabelle, though of course much remains to be added and many improvements are possible.

\section{Probabilistic Prelimilaries}

Readers familiar with the basics of measure-theoretic probability theory may wish to skip to the next section, though this section still serves to establish notation. Readers lacking this background will find a brief introduction here, just sufficient to give an idea of what concepts are behind the proof of the central limit theorem in the next section. Those who wish to learn the measure-theoretic foundations of probability theory should consult Billingsley or other standard works on the subject.

We begin with an explication of the idea of a measure. It is intuitively obvious that some sets of points have a definite ``size:'' A line segment has a length, a circle has an area, a cone has a volume, etc. The notion of a measure is intended to make precise this intuitive notion of the ``size'' of a set. We shall see later how probabilities are interpreted in terms of measures.

\begin{definition}
Let $X$ be a set, and $\Sigma \subseteq X$ be a collection of subsets of $X$ which contains $\emptyset$ and is closed under complements and countable unions (from which it immediately follows that $X \in \Sigma$ and that $\Sigma$ is closed under countable intersections). A {\em measure} on $\Sigma$ (often simply called a measure on $X$ when the intended collection $\Sigma$ is clear from the context) is a function $\mu\colon \Sigma \rightarrow [0, \infty]$ with the property that $\mu(\emptyset) = 0$ and which is countably additive, which means that for every pairwise disjoint collection $\bldset{A_n}{n \in \N}$ of elements of $\Sigma$,
\[ \mu\left(\bigcup_{n = 0}^\infty A_n\right) = \sum_{n=0}^\infty \mu(A_n). \]
\end{definition}

For $A \in \Sigma$ as in the definition, the value $\mu(A)$ is called the {\em measure} of $A$ and corresponds to the intuitive notions of size, length, area, volume, etc. Note that the measure of a set may be infinite; indeed, if $\mu$ is a measure on $\R$ such that $\mu((n, n+1]) = 1$ for $n \in \N$ (as should be the case if $\mu$ measures the length of intervals), it is immediate from the definition of a measure that $\mu(\R) = \infty$.

A collection $\Sigma$ of subsets of a set $X$ satisfying the hypothesis in the above definition (i.e. containing $\emptyset$ and closed under complements and countable unions) is called a $\sigma$-algebra. The elements of $\Sigma$ are called the {\em measurable sets,} and a set with an associated $\sigma$-algebra but no associated measure is called a {\em measurable space.} 

One might ask why a measure should not determine a size for every subset of $X$; one important reason is that there is no translation-invariant measure $\mu$ on $\R$ which assigns intervals the expected length (i.e. $\mu [a,b] = b - a$ for $a < b$) and is defined for all subsets of $\R$. Of course, the zero measure on a set is the measure which assigns measure zero to every subset of $X$, and a measure on $\R$ is called {\em translation-invariant} iff for every measurable $A$ and every $x \in \R$, $\mu(A + x) = \mu(A)$, where $A + x$, the translate of $A$ by $x$, is $\bldset{a + x}{a \in A}$. Suppose now for contradiction that $\mu$ is a translation-invariant measure on $\R$ which measures all subsets of $\R$ (so the measurable space on which $\mu$ is defined is $(\R, \mathcal P(\R))$, where $\mathcal P(\R)$ is the powerset of $\R$) and satisfies $\mu [a,b] = b - a$ for $a < b$. Consider the equivalence relation $\sim$ on $\R$ defined by $x \sim y$ iff $|x - y| \in \Q$. Because the rationals are dense in $\R$ it is clear that each equivalence class contains an element of the closed unit interval. Using the axiom of choice, select one element of the intersection of each equivalence class with $[0,1]$ (this use of the axiom of choice is essential; there are models of set theory where there exists a translation-invariant measure $\lambda$ defined for all subsets of $\R$ and assigning intervals the expected length). Denote this collection by $V = \bldset{r_\alpha}{\alpha \in I}$, where $I$ is some index set. Enumerate the rationals in $[-1,1]$ as $\bldset{q_n}{n \in \N}$, and for each $n$ define $V_n = V + q_n$. Since $\mu$ is translation-invariant, all elements of the collection $\bldset{V_n}{n \in \N}$ receive the same measure. Let $E = \bigcup_{n \in \N} V_n$; it is clear from the definition of the $V_n$'s that $[0,1] \subseteq E \subseteq [-1,2]$. Furthermore, it is an immediate consequence of countable (in this case finite) additivity that for any measure $\nu$ on any space, if $A$ and $B$ are measurable and $A \subseteq B$, then $\nu(A) \le \nu(B)$ (note $B = A \cup (B \setminus A)$ and this union is disjoint). Hence because $\mu$ assigns intervals their expected length, we have $1 \le \mu(E) \le 3$. However, by countable additivity

\[ \mu(E) = \mu\left(\bigcup_{n=0}^\infty V_n\right) = \sum_{n=0}^\infty \mu(V_n). \]

Since $\mu(V_n)$ is the same for all $n$, the sum on the right is infinite if it is not zero, and we have obtained a contradiction. This counterexample is due to Vitali.\todo{Add citation.}

The standard solution to the nonexistence problem noted in the preceding paragraph is to restrict which sets are assigned measures--hence the $\sigma$-algebra $\Sigma$ in the definition of a measure. This allows ``bad'' sets such as $V$ from the counterexample to be excluded from receiving a measure, and is essential to a useful theory of measure.

Since measures extend the notion of length of intervals, it is natural to suppose that all intervals should be measurable (let us say open intervals, for definiteness). If all open intervals are measurable, then all sets in the $\sigma$-algebra generated by the open intervals--the intersection of all $\sigma$-algebrae containing all the open intervals, an object which is easily verified to be a $\sigma$-algebra--must also be meaurable. The $\sigma$-algebra generated by all open intervals in $\R$ is called the {\em Borel} $\sigma$-algebra on $\R$, and more generally for any topological space the associated Borel $\sigma$-algebra is the $\sigma$-algebra generated by the open sets. Most\todo{Decide whether ``all'' is accurate.} measures encountered in our formalization are Borel measures.

How does this relate to probability? Well, in probability theory, one wishes to assign probabilities to events. The probability that a fair coin turns up heads should be $\frac{1}{2}$, the probability that a randomly selected element of the unit interval $[0,1]$ is between $\frac{1}{3}$ and $\frac{2}{3}$ should be $\frac{1}{3}$, etc. How can these events be modelled formally? Let $\Omega$ be the set of all possible states of the world (or simply the set of all possible worlds, if one is of a philosophical bent); an event is simply a collection of such states (namely the collection where the event occurs). Thus for the toss of a fair coin we may take $\Omega = \{H, T\}$, where $H$ is a world where the coin turns up heads, and $T$ a world where it turns up tails. The event that the coin turns up heads is simply $\{H\}$. Similarly, for randomly selecting an element of the unit interval we may take $\Omega = [0,1]$, $\omega$ being a world where the selected element is $\omega$ for each $\omega \in \Omega$. The event that the selected number is between $\frac{1}{3}$ and $\frac{2}{3}$ is then simply $(\frac{1}{3}, \frac{2}{3})$. The space $\Omega$ is called the {\em sample space} in probability theory.

It is intuitively clear that the probability of the impossible event $\emptyset$ is zero, and that the probability of a union of disjoint events should be the sum of their probabilities. It is therefore reasonable to suppose probability determines a measure, $\P$, on some collection of events (which we might consider ``observable''). Vacuously $\emptyset$ is observable, and it is clear that if an event $E$ is observable then so should be its complement, and that if events $\bldset{E_n}{n \in \N}$ are observable then so should be their union. Thus the collection of observable events should be a $\sigma$-algebra. It should be noted that in probability theory, the term ``event'' is generally reserved for what we have termed ``observable events,'' and we shall follow this convention in the sequel.

A feature of measures determined by probabilities of events is that $\P(X) = 1$; probabilities cannot be arbitrarily large, and it is assumed to be certain that something in the sample space will occur (so the sample space is exhaustive). A measure is called {\em finite} if $\P(X) < \infty$, and modulo the zero measure the theory of finite measures is the same as the theory of probability measures (any nonzero finite measure $\mu$ can be scaled by $\frac{1}{\mu(X)}$ to obtain a probability measure).

\section{Summary of the Proof}

Before finally diving into the details of the formalization, we pause to give a quick overview of how the proof will succeed. Details can be found in Billingsley.\todo{Add citation}

If $X$ and $Y$ are independent random variables with distributions $\mu$ and $\nu$, respectively, the distribution of their sum is the convolution of their distributions: $X + Y \sim \mu * \nu$. Thus if $\bldseq{X_k}{k \le n}$ is a sequence of independent random variables all with distribution $\mu$, the sum $\sum_{k \le n} X_k$ is distributed as the $n$-fold convolution of $\mu$ with itself. However, this $n$-fold convolution is a technically inconvenient object to work with, and to study the asymptotic distribution of $\sum{k \le n} X_k$ as $n \rightarrow \infty$, it turns out to be technically advantageous to take Fourier transforms of the random variables, or to put this in probabilistic language, to study their characteristic functions. The advantage of this method of characteristic functions is twofold: first, if $X$ and $Y$ are independent then the characteristic function of $X + Y$ is simply the product of the characteristic function of $X$ and that of $Y$; and secondly, a sequence $\bldseq{X_n}{n \in \N}$ converges in distribution to a random variable $X$ if and only if the corresponding characteristic functions converge pointwise. This is a significant advantage because products are far easier to work with than convolutions, as is pointwise convergence easier than convergence in distribution. This shift of focus from random variables to their characteristic functions is justified by the L\'evy inversion theorem, which states that two random variables with the same characteristic function have the same distribution.

The proof of the central limit theorem proceeds as one might expect, given this framework: The characteristic function of the normalized sum of $n$ independent identically distributed (normalized) random variables satisfying an appropriate growth condition is shown to converge pointwise to the characteristic function of the standard normal distribution. Proving this is reasonably straightforward, though it requires many delicate estimates based on Taylor series and complex variables, all of which require rather tedious formalization. The bulk of the work in our formalization, however, was in supporting the use of characteristic functions to study the distributions of sums of independent random variables, by proving the L\'evy inversion and continuity theorems.

The proof of the L\'evy continuity theorem employs something akin to kernel methods from harmonic analysis, using the function
\[ \Si(x) = \int_0^x \frac{\sin x}{x} \]
to ``concentrate'' near points of interest. This requires, among other things, proving that $\lim_{x \rightarrow \infty} \Si(x) = \pi/2$, which we computed using Fubini's theorem (see Billingsley pp. 235--236) and required tedious verification of many ``obvious'' facts about integrals (including the validity of changing variables). Deriving uniqueness from the L\'evy continuity theorem required using the fact that the complement of a countable subset of $\R$ is dense in $\R$ and the $\pi$-$\lambda$ theorem from measure theory (the latter fortunately having already been formalized).

Verification of the L\'evy continuity theorem required proving the portmanteau theorem, more calculations with integrals (and another use of Fubini's theorem), and use of the theory of tightness of sequences of probability measures. The portmanteau theorem establishes that convergence in distribution is equivalent to weak* convergence in the sense of functional analysis, and that this is in turn equivalent to pointwise convergence of the measures of sets which contain no atoms of the limit distribution. The proof of the portmanteau theorem uses Skorohod's theorem, which states that if a sequence $\bldseq{\mu_n}{n \in \N}$ of probability measures converges in distribution to a probability measure $\mu$, then there exists a sequence of random variables $\bldseq{X_n}{n \in \N}$ and a random variable $X$, all defined on a common probability space, such that $X_n \sim \mu_n$, $X \sim \mu$, and $X_n \rightarrow X$ pointwise. The proof of this result requires only elementary analysis.

The theory of tightness of measures gives an analogue in the space of probability measures of the Weierstrass theorem that every bounded sequence of reals has a convergent subsequence. Tightness is the requisite analogue of boundedness: Roughly a sequence of probability measures is called tight iff no mass ``escapes to infinity.'' The sequence $\bldseq{\mu_n}{n \in \N}$, where $\mu_n$ is a unit mass at $n$, gives an example of how mass can ``escape to infinity,'' and naturally is an example of failure of tightness. The key result regarding tightness of a sequence $\bldseq{\mu_n}{n \in \N}$ of probability measures is that it is equivalent to the condition that for every subsequence $\bldseq{\mu_{n_k}}{k \in \N}$ there exists a subsubsequence $\bldseq{\mu_{n_{k_j}}}{j \in \N}$ which converges in distribution to some probability measure. A corollary of this result is that if a sequence $\bldseq{\mu_n}{n \in \N}$ is tight, and it can be shown that every subsequence which has a weak limit must converge in distribution to a given probability measure $\mu$, then in fact $\mu_n \Rightarrow \mu$.

The proof of the main result concerning tight sequences of measures requires the Helly selection theorem, which is of importance also in functional analysis.This is another analogue of Weierstrass theorem, this time giving that if $\bldseq{F_n}{n \in \N}$ is a sequence of distribution functions then it has a subsequence $\bldseq{F_{n_k}}{k \in \N}$ which converges vaguely to some nondecreasing, right-continuous function $F$ (which may not be a distribution function because its limit at $\infty$ may be less than $1$). This is proven using the method of diagonal subsequences to obtain values for the requisite function $F$ at rationals, and extending to all reals by right-continuity. The elementary analytical arguments involved in this were straightforward but tedious to verify.

\section{Overview of Isabelle}

\section{The Formalized Proof}

\subsection{Distribution Functions}

Often it is more convenient to work with a real-valued function determining a measure on $\R$ than directly with the measure, and an obvious way to accomplish this for finite measures is to study the distribution function of the measure, which when evaluated at an argument gives the amount of mass below that argument.

\begin{definition}
Let $\mu$ be a finite measure on $\R$. The (cumulative) distribution function $F_\mu$ is defined by $F_\mu(x) = \mu (-\infty, x]$.
\end{definition}

Before proving that the distribution function of a measure uniquely determines that measure, let us note some general properties of distribution functions. For convenience we assume the measures we are working with are probability measures; other nonzero finite measures can be normalized to probability measures, and the zero measure is trivial.

\begin{theorem}
The distribution function $F_\mu$ of a finite measure $\mu$ is nondecreasing and right-continuous and satisfies $\lim_{x \rightarrow -\infty} F_\mu(x) = 0$ and $\lim_{x \rightarrow \infty} F_\mu(x) = 1$.
\end{theorem}

$F_\mu$ nondecreasing follows from nonnegativity of $\mu$; right-continuity follows from continuity of $\mu$ from below as if $x_n \uparrow x$ then $(-\infty
, x_n] \uparrow (-\infty, x]$. $\lim_{x \rightarrow -\infty} F_\mu(x) = 0$ follows from continuity of $\mu$ from above as $(-\infty, -n] \downarrow \emptyset$, and $\lim_{x \rightarrow \infty} F_\mu(x) = 1$ follows from continuity of $\mu$ from below as $(-\infty, n] \uparrow \R$.

\todo[inline]{Give Isabelle proof script snippets.}

In turn, any function with the properties listed in the preceding theorem is the distribution function of a probability measure on $\R$:

\begin{theorem}
Suppose $F\colon \R \rightarrow \R$ is nondecreasing, right-continuous, and satisfies $\lim_{x \rightarrow -\infty} F(x) = 0$ and $\lim_{x \rightarrow \infty} F(x) = 1$. Then there exists a Borel probability measure $\mu$ on $\R$ such that $F = F_\mu$.
\end{theorem}

The requisite measure $\mu$ is constructed by defining $\mu (a,b] = F(b) - F(a)$ and extending this to the Borel $\sigma$-algebra using the Carath\'eodory extension theorem, which fortunately Johannes H\"olzl had already formalized. 

\todo[inline]{Decide whether to rewrite the following as a displayed theorem.}
A similar method of proof also gives that the distribution function of a probability measure is unique in the sense that if $F_\mu = F_\nu$ then $\mu = \nu$, because $F_\mu = F_\nu$ implies $\mu (a,b] = \nu (a,b]$ for every $a,b$, and the half-open intervals are a $\pi$-system generating the Borel sets on $\R$, so $\mu = \nu$ by Dynkin's uniqueness lemma.

\todo[inline]{Isabelle proof script snippets.}

\subsection{Weak Convergence}



\subsection{Helly Selection Theorem and Tightness}



\subsection{Integration}

The proofs of the L\'evy inversion and continuity theorems required formal work with integrals, and brought a number of design issues into focus. First, one frequently wishes to integrate a function only over a subset $A$ of the space on which it is defined rather than the whole space. This can be handled in two ways: The integral over the whole space can be taken as primitive, with the integral of a function $f$ over a set $A$ defined by $\int_A f \, d\mu = \int f \mathbbm{1}_A \, d\mu$,\todo{Ensure indicator function notation is defined prior to this point.} or the integral over a set can be taken as primitive with the integral of a function $f$ over the whole space being defined by $\int f \, d\mu = \int_X f \, d\mu$.
\todo[inline]{List advantages and disadvantages, and give rationale behind Isabelle method.}

One particular type of set occurs particularly frequently as the domain of integration with respect to Lebesgue measure on $\R$, namely a closed interval. In calculus the integral (with respect to Lebesgue meausre) of a function $f$ over a closed interval $[a,b]$ ($a \le b$) is denoted $\int_a^b f \, dx$, and it is convenient to have a similar notation for integrals over intervals in Isabelle. A number of design issues immediately present themselves: What should be the types of $a$ and $b$? One might assume these should be reals, but frequently integrals are computed over unbounded intervals ($\int_0^\infty e^{-x} \, dx$, $\int_{-\infty}^\infty e^{-x^2} \, dx$), and so there is advantage to taking $a$ and $b$ to be extended reals to avoid the need for separate lemmata for integrals of the form $\int_a^\infty$, $\int_{\infty}^b$ and $\int_{-\infty}^\infty$ (this last form being a notational variant of $\int$). However, this advantage is achieved at a cost, because it entails annoying casts between reals and extended reals that we found in practice frequently prevent automated tools from proving apparently obvious facts (which they do in fact obtain when the endpoints are taken to be of type real).

As noted in the preceding paragraph, for interval integrals with respect to Lebesgue measure the interval is generally assumed to be closed (so any continuous function defined on it is uniformly continuous, and other nice properties hold); since Lebesgue measure is continuous, it makes no difference to the value of the integral whether the endpoints are included. However, for general measures this does make a difference if one of the endpoints is an atom, and in particular the interval partition formula, valid for $f$ integrable with respect to Lebesgue measure and $a \le b \le c$:
\[ \int_a^b f \, dx + \int_b^c f \, dx = \int_a^c f \, dx \]
fails in general if $\int_a^b$ is defined as $\int_{[a,b]}$. What holds instead for arbitrary measures $\mu$ (and functions $f$ integrable over $[a,c]$ with respect to $\mu$) is that
\[ \int_a^b f \, d\mu + \int_b^c f \, d\mu = \int_a^c f \, d\mu + f(b)\mu \{b\}. \]
Intuitively this is because the mass of the point $b$ was counted twice. This is obviously less convenient than the ordinary interval partition formula, especially for partitions into large numbers of pieces. The problem can be fixed by using a half-open interval to ensure elements of an interval partition are disjoint; such a solution preserves the equality $\int_a^b f \, d\mu = \int_{[a,b]} f \, d\mu$ for continuous measures, and satisfies the intuitive partition formula
\[ \int_a^b f \, d\mu + \int_b^c f \, d\mu = \int_a^c f \, d\mu \]
for all $a,b,c$ with $a < b < c$.
\todo[inline]{Figure out why Bochner integration library does not use half-open intervals, and instead open intervals, which have the same problem as closed intervals.}

A further constellation of design issues arises from considering what should happen if $b < a$ in $\int_a^b f \, dx$. The natural option is to take $\int_a^b f \, dx = -\int_b^a f \, dx$ in this case, but this would require introducing a case split in the definition of $\int_a^b f \, dx$ (e.g. $\int_a^b f \, dx = \int_{[a,b]} f \, dx$ if $a \le b$, otherwise $\int_a^b f \, dx = -\int_{[b,a]} f \, dx$), which then causes headaches for formalization both for human users and automated tools. $\int_a^b f \, dx$ could also be taken simply to be notation for $\int_{[a,b]} f \, dx$, in which case $b < a$ implies $[a,b] = \emptyset$ and so the integral is zero, but this makes it hard to state results such as the fundamental theorem of calculus or the theorem regarding change of variables in a natural manner.

Another set of issues concern integrability. Not all functions have integrals, and the notation $\int f \, d\mu$ only makes sense if $f$ is integrable (over $X$). Isabelle requires that all functions be total, so $\int f \, d\mu$ necessarily has a value no matter what $f$ is. If $f$ is not integrable, the value which $\int f \, d\mu$ receives is arbitrary. A proof that $\int f \, d\mu = c$ is useless unless $f$ is known to be integrable (for otherwise it could be that $c$ just happens to be the default value in the case of integrating $f$). These are general problems concerning the representation of partial functions, and we shall pause to briefly consider them in full generality.

A partial function with domain $X$ and codomain $Y$ can be thought of as a relation $R \subseteq X \times Y$ such that for every $x \in X$ and $y, z \in Y$, $xRy$ and $xRz$ implies $y=z$ (a [total] function corresponds to such a relation where in addition for every $x \in X$ there exists $y \in Y$ such that $xRy$, though the higher-order logic used by Isabelle treats functions somewhat differently [not as relations]). Let $y^*$ be some distinguished element of the type of elements of $Y$, and consider the function $f\colon X \rightarrow Y \cup \{y^*\}$ where $f(x) = y$ if there exists $y \in Y$ such that $xRy$, and otherwise $f(x) = y^*$. The value $y^*$ should be thought of as hidden; the fact that $f(x) = y^*$ if there does not exist $y \in Y$ such that $xRy$ should not be exploited in proofs. In this case the conclusion that $f(x) = y$ is useless unless it is known that there exists $y \in Y$ such that $xRy$. Since the existence of $y \in Y$ such that $xRy$ means intuitively that $f$ is ``defined'' at $x$, let us denote it by $Dx$. Then $xRy$ is equivalent to $Dx$ and $f(x) = y$. Since $f(x) = y$ is useless without knowing $Dx$, the conclusions of computations of $f$ for various arguments should either be stated in terms of $R$ or have the auxilliary conclusion that $Dx$ for each argument $x$ of $f$ considered in the computation. $xRy$ is a robust conclusion and functions well as the fundamental notion in terms of which $f$ and $D$ are defined, while it is convenient to have $f$ both to allow statement of results in a manner more similar to mathematical practice, and to allow more convenient computation with values of the partial function (e.g. $f(x_1) + f(x_2)$ is easier to work with than \texttt{(THE $y$.\!\!\! $x_1Ry$) + (THE $y$.\!\!\! $x_2Ry$)} or something like that.

The Isabelle libraries we used during the formalization of the central limit theorem employed and integral operator and an integrability predicate; there was no instantiation of the relation $R$ from the preceding paragraph. This could have worked had every use of the integral operator been accompanied by a corresponding proof of integrability, but unfortunately this was not the case, and sometimes we had to repeat long arguments from library lemmata with trivial modifications so as to obtain integrability. Another difficulty was that when working with definite functions, one often wishes to prove integrability by computing the integral (e.g. the integral of $e^{-x}$ over $[0, \infty)$ is $1$), and this is generally very inconvenient to do when integrability must be verified separately. A better plan in such cases is to have an instantiation \texttt{has\_integral} of the relation $R$ from the preceding paragraph, and prove integrability by computing the integral by proving that \texttt{$f$ has\_integral $c$} for appropriate $c$. Thus it is convenient to have not only an integral operator and an integrability predicate, but also a \texttt{has\_integral} relation. In some sense it does not matter which is taken as fundamental, but as noted in the preceding paragraph it seems more natural to take \texttt{has\_integral} as fundamental.

The change from a fundamental integral operator and integrability predicate to a fundamental \texttt{has\_integral} relation was accomplished by Johannes H\"olzl when reimplementing the integration library using the Bochner integral. This change was motivated largely by a desire to provide a unified framework for vector-valued integrals, and was of direct importance to the central limit theorem formalization because the complex-valued integrals arising from characteristic functions can be handled as Bochner integrals.

There is also the question of how to handle improper integrals; for example, the $\sinc$ function is not integrable over $[0,\infty)$, and yet 
\[ \lim_{t \rightarrow \infty} \int_0^t \sinc x \, dx = \frac{\pi}{2}. \]
Often this is written simply as $\int_0^\infty \sinc x \, dx = \pi/2$, perhaps with a warning that notation is being abused (see note regarding this limit in \cite{Billingsley}, p. 223). Improper integrals also occur over finite intervals, for example
\[ \lim_{t \rightarrow 0} \int_t^1 \frac{(-1)^{\floor x + 1}}{\floor x} \mathbbm 1_{(0,1]} \, dx = \ln 2, \]
but the integrand is not integrable over $(0,1]$ because the harmonic series diverges. It would be possible to implement the integral over an interval to include improper integrals, as is standard practice in calculus texts, by including a limit in the definition of such an integral. However, this seems to carry with it too many disadvantages. One is that for measures with atoms, the integral over a half-open interval $(a,b]$ ($a < b$) is not in general equal to the integral over the closed interval $[a,b]$ (in particular, for any function $f$ integrable over $[a,b]$ and any measure $\mu$ on $\R$, $\int_{[a,b]} f \, d\mu = \int_{(a,b]} f \, d\mu + \mu \{a\} f(a)$). Another is simply the complication that a hidden limit introduces; it is inconvenient for users to constantly need to eliminate the limit whenever they use integrals, and difficult to set up automated tools to deal with this effectively.

\section{Conclusion: Opportunities for Improvement and Extension}

\bibliographystyle{plain}
\bibliography{itp}

\end{document}
