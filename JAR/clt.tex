\documentclass{svjour3}

\usepackage{amsfonts}
\usepackage{amsbsy,amssymb,amsmath}
\usepackage{url}
\usepackage{color}
\usepackage{isabelle,isabellesym}

\newcommand{\todo}[1]{{\color{red}#1}}

\newcommand{\NN}{\mathbb{N}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\CC}{\mathbb{C}}

\newcommand{\fn}[1]{\mathtt{#1}} % for functions
\newcommand{\mdl}[1]{{\mathcal #1}} % for the sigma algebras
\newcommand{\ph}{\varphi}

\newcommand{\sinc}{\mathop{\fn{sinc}}\nolimits}

% \hypersetup{
%     colorlinks=true,
%     citecolor=blue,
%     linkcolor=blue,
%     urlcolor=blue
% }


\begin{document}

% first the title is needed
\title{A formally verified proof of the Central Limit Theorem}

%\titlerunning{}  % abbreviated title (for running head)

\author{Jeremy Avigad \and Johannes H\"olzl \and Luke Serafin}
\authorrunning{J.~Avigad, J.~H\"olzl, and L.~Serafin}   % abbreviated author list (for running head)

\institute{J. Avigad \and L. Serafin \at Carnegie Mellon University \and J. H\"olzl \at Technische Universit\"at M\"unchen}

\maketitle

\begin{abstract}
We describe a formally verified proof of the Central Limit Theorem in the Isabelle proof assistant, which builds upon and extends Isabelle's libraries for analysis and measure-theoretic probability. 
% The statement of the theorem relies on the notion of \emph{weak convergence}, also known as \emph{convergence in distribution}. 
The proof of the theorem uses \emph{characteristic functions}, which are a kind of Fourier transform, to demonstrate that, under suitable hypotheses, sums of random variables converge weakly to the standard normal distribution. We also discuss the libraries and infrastructure that supported the formalization.
\keywords{interactive theorem proving, measure theory, central limit theorem}
\end{abstract}


\section{Introduction}
\label{section:introduction}

If you roll a fair die many times and compute the average number of spots showing, the result is likely to be close to 3.5, and the odds that the average is far from the expected value decreases roughly as the area under the familiar bell-shaped curve. Something similar happens if the measurement is continuous rather than discrete, such as when you repeatedly toss a needle on the ground and measure the angle it makes with respect to a fixed reference line. Even if the die is not a fair die or the geometry of the needle and the ground makes some angles more likely than others, the distribution of the average still approaches the area under a bell-shaped curve centered on the expected value. The width of the bell depends on both the variance of the random measurement and the number of times it is performed. Made precise, this amounts to a statement of the Central Limit Theorem.

The Central Limit Theorem lies at the heart of modern probability. Many generalizations and variations have been studied, relaxing the requirement that the repeated measurements are independent of one another and identically distributed (cf.~in particular, the results of Lyapunov and Lindberg, described, for example, in \cite{billingsley:95}), or providing additional information on the rate of convergence.

Here we report on a formalization of the Central Limit Theorem that was carried out in the Isabelle proof assistant. This result is noteworthy for a number of reasons. Not only is the Central Limit Theorem fundamental to probability theory and the study of stochastic processes, but so is almost all of the machinery developed to prove it, ranging from ordinary calculus to the properties of real distributions and characteristic functions. There is a pragmatic need to have statistical claims made in engineering, risk analysis, and financial computation subject to formal verification, and our formalization along with the surrounding infrastructure supports such practical efforts.

The formalization is also a good test for Isabelle's libraries, proof language, and automated reasoning tools. As will become clear below, the proof draws on a very broad base of facts from analysis, topology, measure theory, and probability theory, providing a useful evaluation of the robustness and completeness of the supporting libraries. Moreover, the concepts build on one another. For example, a measure is a function from a class of sets to the reals, and reasoning about convergence of measures involves reasoning about sequences of such functions. The operation of forming the characteristic function is a functional taking a measure to a function from the reals to the complex numbers, and the convergence of such functionals is used to deduce convergence of measures. The conceptual underpinnings are thus as deep as they are broad, and working with the notions exercises Isabelle's mechanisms for handling abstract mathematical notions.

In Section~\ref{section:overview}, we provide an overview of the Central Limit Theorem, and the proof that we formalized, following the textbook presentation of Billingsley~\cite{billingsley:95}. In Section~\ref{section:isabelle}, we describe the Isabelle proof assistant, and the parts of the library that supported our formalization. In Section~\ref{section:formal}, we describe the formal proof itself, and in Section~\ref{section:reflections}, we reflect on what we have learned from the effort. 

Our formalization is currently part of the Isabelle library, which can be found online at \url{https://isabelle.in.tum.de/}.\footnote{The probability library in particular can be found at \url{https://isabelle.in.tum.de/dist/library/HOL/HOL-Probability/index.html}.} A preliminary, unpublished report on the formalization can be found on arXiv \cite{avigad:hoelzl:serafin:14}. This report also draws heavily on Serafin's Carnegie Mellon MS thesis \cite{serafin:15}, which provides additional information.

\emph{Acknowledgments.} We are grateful to Tobias Nipkow, Lawrence Paulson, Makarius Wenzel, and the entire Isabelle team for the ongoing development of Isabelle, and to Tobias in particular for steadfast encouragement and support.

\section{Overview of the Central Limit Theorem}
\label{section:overview}

For our formalization, we followed Billingsley's textbook, \emph{Probability and Measure} \cite{billingsley:95}, which provides an excellent introduction to these topics. Here we briefly review the core concepts, give a precise statement of the Central Limit Theorem, and present an outline of the proof.

\subsection{Historical background}
\label{subsection:historical}

In 1733, De Moivre privately circulated a proof that, as $n$ approaches infinity, the distribution of $n$ flips of a fair coin converges to a normal distribution. This material was later published in the 1738 second edition of his book {\em The Doctrine of Chances,} the first edition of which was first published in 1712 and is widely regarded as the first textbook on probability theory. De Moivre also considered the case of what we might call a biased coin (an event which has value one with probability $p$ and zero with probability $1-p$, for some $p \in (0,1)$), and realized that his convergence theorem continues to hold in that case.

De Moivre's result was generalized by Laplace in the period between about 1776 and 1812 to sums of random variables with various other distributions, such as the uniform distribution on an interval. Over the next three decades Laplace developed conceptual and analytical tools to extend this convergence theorem to sums of independent identically distributed random variables with ever more general distributions, and this work culminated in his treatise {\em Th\'eorie analytique des probabilit\'es}. This included the development of the method of characteristic functions to study the convergence of sums of random variables, a move which firmly established the usefulness of analytic methods in probability theory (in particular, the use of characteristic functions, a form Fourier analysis that will be discussed below). 

Laplace's theorem later became known as the Central Limit Theorem, a designation due to P\'olya, stemming from its importance both in the theory and applications of probability. In modern terms, the theorem states that the normalized sum of a sequence of independent and identically distributed random variables with finite, nonzero variance converges to a normal distribution. All of the main ingredients of the proof of the central limit theorem are present in the work of Laplace, though of course the theorem was refined and extended as probability underwent the radical changes necessitated by its move to measure-theoretic foundations in the first half of the twentieth century.

Gauss was one of the first to recognize the importance of the normal distribution to the estimation of measurement errors. The usefulness of the normal distribution in this context is largely a consequence of the Central Limit Theorem, since errors occurring in practice are frequently the result of many independent factors which sum to an overall error in a way which can be regarded as approximated by a sum of independent and identically distributed random variables. The normal distribution also arose with surprising frequency in a wide variety of empirical contexts, from the heights of men and women to the velocities of molecules in a gas. This gave the Central Limit Theorem the character of a natural law, as seen in the following poetic quote from Sir Francis Galton in 1889 \cite{galton:89}:
\begin{quote}
 I know of scarcely anything so apt to impress the imagination as the wonderful form of cosmic order expressed by the ``Law of Frequency of Error.'' The law would have been personified by the Greeks and deified, if they had known of it. It reigns with serenity and in complete self-effacement, amidst the wildest confusion. The huger the mob, and the greater the apparent anarchy, the more perfect is its sway. It is the supreme law of Unreason. Whenever a large sample of chaotic elements are taken in hand and marshaled in the order of their magnitude, an unsuspected and most beautiful form of regularity proves to have been latent all along.
\end{quote}
More details on the history of the central limit theorem and its proof can be found in \cite{fischer:11}.

\subsection{Background from measure theory}
\label{subsection:background}

A \emph{measure space} $(\Omega, \mdl F)$ consists of a set $\Omega$ and a \emph{$\sigma$-algebra $\mdl F$ of subsets of $\Omega$}, that is, a collection of subsets of $\Omega$ containing the empty set and closed under complements and countable unions. Think of $\Omega$ as the set of possible states of affairs, or possible outcomes of an action or experiment, and each element $E$ of $\mdl F$ as representing the set of states or outcomes in which some \emph{event} occurs --- for example, that a card drawn is a face card, or that Spain wins the World Cup. A \emph{probability measure} $\mu$ on this space is a function that assigns a value $\mu(E)$ in $[0, 1]$ to each event $E$, subject to the following two conditions:
\begin{enumerate}
 \item $\mu(\emptyset) = 0$, and
 \item $\mu$ is countably additive: if $(E_i)$ is any sequence of disjoint events in $\mdl F$, $\mu(\bigcup_i E_i) = \sum_i \mu(E_i)$.
\end{enumerate}
Intuitively, $\mu(E)$ is the ``probability'' that $E$ occurs. 

The collection $\mdl B$ of \emph{Borel subsets} of the real numbers is the smallest $\sigma$-algebra containing all intervals $(a, b)$. A \emph{random variable} $X$ on the measure space $(\Omega, \mdl F)$ is a measurable function from $(\Omega, \mdl F)$ to $(\RR, \mdl B)$. Saying $X$ is measurable means that for every Borel subset $B$ of the real numbers, the set $\{ \omega \in \Omega \; | \; X(\omega) \in B \}$ is in $\mdl F$. Think of $X$ as some real-valued measurement that one can perform on the outcome of the experiment, in which case, the measurability of $X$ means that if we are given any probability measure $\mu$ on $(\Omega, \mdl F)$, then for any Borel set $B$ it makes sense to talk about ``the probability that $X$ is in $B$.'' In fact, if $X$ is a random variable, then any measure $\mu$ on $(\Omega, \mdl F)$ gives rise to a measure $\nu$ on $(\RR, \mdl B)$, defined by $\nu(B) = \mu ( \{ \omega \in \Omega \; | \; X (\omega) \in B \})$. A measure on $(\RR, \mdl B)$ is called a \emph{real distribution}, or, more simply, a \emph{distribution}, and the measure $\nu$ just described is called \emph{the distribution of $X$}.

If $X$ is a random variable, the \emph{mean} or \emph{expected value} of $X$ with respect to a probability measure $\mu$ is $\int X d\mu$, the integral of $X$ with respect to $\mu$. If $m$ is the mean, the \emph{variance} of $X$ is $\int (X - m)^2 d\mu$, a measure of how far, on average, we should expect $X$ to be from its average value.

Note that passing from $\mu$ and $X$ to its distribution $\nu$ means that instead of worrying about the probability that some abstract event occurs, we focus more concretely on the probability that some measurement on the outcome lands in some set of real numbers. In fact, many theorems of probability theory do not really depend on the abstract space $(\Omega, \mdl F)$ on which $X$ is defined, but, rather, the associated distribution on the real numbers. Nonetheless, it is often more intuitive and convenient to think of the real distribution as being the distribution of a random variable (and, indeed, any real distribution can be represented that way). 

One way to define a real distribution is in terms of a \emph{density}. For example, in the case where $\Omega = \{1, 2, 3, 4, 5, 6\}$, we can specify a probability on all the subsets of $\Omega$ by specifying the probability of each of the events $\{1\}, \{2\}, \ldots, \{6\}$. More generally, we can specify a distribution $\mu$ on $\RR$ by specifying a function $f$ such that for every interval $(a, b)$, $\mu((a, b)) = \int_a^b f x \; \mathit{dx}$. The measure $\mu$ is then said to be the real distribution with density $f$. In particular, the \emph{normal distribution} with mean $m$ and variance $\sigma^2$ is defined to be the real distribution with density function
\[
f(x) = \frac{1}{\sigma \sqrt{2 \pi}} e^\frac{-(x - m)^2}{2 \sigma^2}, 
\]
a ``bell shaped curve'' centered at $m$. When $m = 0$ and $\sigma = 1$, this is called the \emph{standard normal distribution}.

Let $X_0, X_1, X_2, \ldots$ be any sequence of independent random variables, each with the same distribution $\mu$, mean $m$, and variance $\sigma^2$. Here ``independent'' means that the random variables $X_0, X_1, \ldots$ are all defined on the same measure space $(\Omega, \mdl F)$, but they represent independent measurements, in the sense that for any finite sequence of events $B_1, B_2, \ldots, B_k$ and any sequence of distinct indices $i_1, i_2, \ldots, i_k$, the probability that $X_{i_j}$ is in $B_j$ for each $j$ is just the product of the individual probabilities that $X_{i_j}$ is in $B_j$. For each $n$, let $S_n = \sum_{i < n} X_i$. Notice that each $S_n$ is really a measurable function on $(\Omega, \mdl F)$, which is to say, it is a random variable; and so it is natural to ask how its values are distributed. We can shift the expected value of $S_n$ to $0$ by subtracting $n m$, and scale the variance to $1$ by dividing by $\sqrt{ n \sigma^2}$. The Central Limit Theorem says that the corresponding quantity,
\[
 \frac{S_n - nm}{\sqrt{n \sigma^2}},
\]
approaches the standard normal distribution as $n$ approaches infinity.

All that remains to do is to make sense of the assertion that a sequence of distributions $\mu_0, \mu_1, \mu_2, \ldots$ ``approaches'' a distribution, $\mu$. For distributions that are defined in terms of densities, the intuition is that over time the graph of the density should look more and more like the graph of the density of the limit. For example, if you flip a coin a number of times and graph all the possible values of the average number of ones, the discrete points plotted over the possibilities $0, 1/n, 2/n, 3/n, \ldots, 1$ start to look like a bell-shaped curve centered on $1 / 2$. The notion of \emph{weak convergence} makes the notion of ``starts to look like'' precise.

If $\mu$ is any real distribution, then the function $F_\mu(x) = \mu((-\infty, x])$ is called the \emph{cumulative distribution function} of $\mu$. In words, for every $x$, $F_\mu(x)$ returns the likelihood that a real number chosen randomly according to the distribution is at most $x$. Clearly $F_\mu(x)$ is nondecreasing, and it is not hard to show that $F_\mu$ is right continuous, approaches $0$ as $x$ approaches $-\infty$, and approaches $1$ as $x$ approaches $\infty$. Conversely, one can show that any such function is the cumulative distribution function of a unique measure. Thus there is a one-to-one correspondence between functions $F$ satisfying the properties above and real distributions.

The notion of weak convergence can be defined in terms of the cumulative distribution function:
\begin{definition}
 Let $(\mu_n)$ be a sequence of real distributions, and let $\mu$ be a real distribution. Then \emph{$\mu_n$ converge weakly $\mu$}, written $\mu_n \Rightarrow \mu$, if $F_{\mu_n}(x)$ approaches $F_\mu(x)$ at each point $x$ where $F_\mu$ is continuous.
\end{definition}

To understand why we need to exclude points of continuity of $F_\mu$, for each $n$, consider the probability measure $\mu_n$ that puts all its ``weight'' on $1 / n$, which is to say, for any Borel set $B$, $\mu(B) = 1$ if and only if $B$ contains $1 / n$. Then $F_{\mu_n}$ is the function that jumps from $0$ to $1$ at $1 / n$. Intuitively, it makes sense to say that $\mu_n$ approaches the real distribution $\mu$ that puts all its weight at $0$. But for every $n$, $F_{\mu_n}(0) = 0$, while $F_\mu(0) = 1$, which explains why want to exclude the point $0$ from consideration. Notice that since $F_\mu$ is a monotone function, it can have at most countably many points of discontinuity, so we are excluding only countably many points.

The fact that weak convergence is a robust notion is evidenced by the fact that it has a number of equivalent characterizations, as discussed in Section~\ref{subsection:weak:convergence} below.

With this background in place, we can now state the Central Limit Theorem precisely, as follows:
\begin{theorem}
\label{theorem:clt}
Let $X_0, X_1, X_2, \ldots$ be a sequence of independent random variables with mean $0$, variance $\sigma^2$, and common distribution $\mu$. Let $S_n = (X_0 + X_1 + \ldots + X_{n-1})$. Then the distribution of $S_n / \sqrt{n \sigma^2}$ converges weakly to the standard normal distribution.
\end{theorem}
The restriction to mean $0$ does not result in any loss of generality, since if each $X_i$ has mean $m$, we can apply Theorem~\ref{theorem:clt} to the sequence $(X_i - m)$ to obtain the more general statement above. Our formulation of Theorem~\ref{theorem:clt} in Isabelle is as follows:

%depicted as Figure~\ref{fig:clt}.
%\begin{figure}
\begin{quote}
\begin{isabellebody}
\isacommand{theorem}\isamarkupfalse%
\ {\isacharparenleft}\isakeyword{in}\ prob{\isacharunderscore}space{\isacharparenright}\ central{\isacharunderscore}limit{\isacharunderscore}theorem{\isacharcolon}\isanewline
\ \ \isakeyword{fixes}\ \isanewline
\ \ \ \ X\ {\isacharcolon}{\isacharcolon}\ {\isachardoublequoteopen}nat\ {\isasymRightarrow}\ {\isacharprime}a\ {\isasymRightarrow}\ real{\isachardoublequoteclose}\ \isakeyword{and}\isanewline
\ \ \ \ {\isasymmu}\ {\isacharcolon}{\isacharcolon}\ {\isachardoublequoteopen}real\ measure{\isachardoublequoteclose}\ \isakeyword{and}\isanewline
\ \ \ \ {\isasymsigma}\ {\isacharcolon}{\isacharcolon}\ real\ \isakeyword{and}\isanewline
\ \ \ \ S\ {\isacharcolon}{\isacharcolon}\ {\isachardoublequoteopen}nat\ {\isasymRightarrow}\ {\isacharprime}a\ {\isasymRightarrow}\ real{\isachardoublequoteclose}\isanewline
\ \ \isakeyword{assumes}\isanewline
\ \ \ \ X{\isacharunderscore}indep{\isacharcolon}\ {\isachardoublequoteopen}indep{\isacharunderscore}vars\ {\isacharparenleft}{\isasymlambda}i{\isachardot}\ borel{\isacharparenright}\ X\ UNIV{\isachardoublequoteclose}\ \isakeyword{and}\isanewline
\ \ \ \ X{\isacharunderscore}integrable{\isacharcolon}\ {\isachardoublequoteopen}{\isasymAnd}n{\isachardot}\ integrable\ M\ {\isacharparenleft}X\ n{\isacharparenright}{\isachardoublequoteclose}\ \isakeyword{and}\isanewline
\ \ \ \ X{\isacharunderscore}mean{\isacharunderscore}{\isadigit{0}}{\isacharcolon}\ {\isachardoublequoteopen}{\isasymAnd}n{\isachardot}\ expectation\ {\isacharparenleft}X\ n{\isacharparenright}\ {\isacharequal}\ {\isadigit{0}}{\isachardoublequoteclose}\ \isakeyword{and}\isanewline
\ \ \ \ {\isasymsigma}{\isacharunderscore}pos{\isacharcolon}\ {\isachardoublequoteopen}{\isasymsigma}\ {\isachargreater}\ {\isadigit{0}}{\isachardoublequoteclose}\ \isakeyword{and}\isanewline
\ \ \ \ X{\isacharunderscore}square{\isacharunderscore}integrable{\isacharcolon}\ {\isachardoublequoteopen}{\isasymAnd}n{\isachardot}\ integrable\ M\ {\isacharparenleft}{\isasymlambda}x{\isachardot}\ {\isacharparenleft}X\ n\ x{\isacharparenright}\isactrlsup {\isadigit{2}}{\isacharparenright}{\isachardoublequoteclose}\ \isakeyword{and}\isanewline
\ \ \ \ X{\isacharunderscore}variance{\isacharcolon}\ {\isachardoublequoteopen}{\isasymAnd}n{\isachardot}\ variance\ {\isacharparenleft}X\ n{\isacharparenright}\ {\isacharequal}\ {\isasymsigma}\isactrlsup {\isadigit{2}}{\isachardoublequoteclose}\ \isakeyword{and}\isanewline
\ \ \ \ X{\isacharunderscore}distrib{\isacharcolon}\ {\isachardoublequoteopen}{\isasymAnd}n{\isachardot}\ distr\ M\ borel\ {\isacharparenleft}X\ n{\isacharparenright}\ {\isacharequal}\ {\isasymmu}{\isachardoublequoteclose}\isanewline
\ \ \isakeyword{defines}\isanewline
\ \ \ \ {\isachardoublequoteopen}S\ n\ {\isasymequiv}\ {\isasymlambda}x{\isachardot}\ {\isasymSum}i{\isacharless}n{\isachardot}\ X\ i\ x{\isachardoublequoteclose}\isanewline
\ \ \isakeyword{shows}\isanewline
\ \ \ \ {\isachardoublequoteopen}weak{\isacharunderscore}conv{\isacharunderscore}m\ {\isacharparenleft}{\isasymlambda}n{\isachardot}\ distr\ M\ borel\ {\isacharparenleft}{\isasymlambda}x{\isachardot}\ S\ n\ x\ {\isacharslash}\ sqrt\ {\isacharparenleft}n\ {\isacharasterisk}\ {\isasymsigma}\isactrlsup {\isadigit{2}}{\isacharparenright}{\isacharparenright}{\isacharparenright}\ \isanewline
\ \ \ \ \ \ \ \ {\isacharparenleft}density\ lborel\ standard{\isacharunderscore}normal{\isacharunderscore}density{\isacharparenright}{\isachardoublequoteclose}
\end{isabellebody}
\end{quote}
%\caption{The Central Limit Theorem}
%\label{fig:clt}
%\end{figure}

\subsection{An overview of the proof}
\label{subsection:overview}

Contemporary proofs of the Central Limit Theorem rely on the use of \emph{characteristic functions}, a powerful method that dates back to Laplace. If $\mu$ is a real-valued distribution, its characteristic function $\ph(t)$ is defined by
\[
\ph(t) = \int_{-\infty}^{\infty} e^{itx} \mu(dx).
\]
In words, $\ph(t)$ is the integral of the function $f(x) = e^{itx}$ over the whole real line, with respect to the measure $\mu$. Notice that for each $t \neq 0$, the function $e^{itx}$ is periodic with period $2 \pi / t$. It might be helpful to think of $e^{itx}$, as a function of $x$, as like a sine or cosine in $x$ whose period depends on $t$. (Indeed, $e^{itx}= \cos (t x) + i \sin (t x)$.) Notice that $\ph(0) = 1$, the measure of the entire real line. The characteristic function of a real distribution $\mu$ is a Fourier transform of the measure $\mu$, and when $t \neq 0$, $\ph(t)$ ``detects'' periodicity in the way that the real distribution $\mu$ distributes its ``weight'' over different parts of the real line.

A key fact is that if $X_1$ and $X_2$ are independent random variables, then the characteristic function of $X_1 + X_2$ is the product of the characteristic function of $X_1$ and the characteristic function of $X_2$. Of course, this extends to sums with any finite number of terms, and the resulting products are often easier to work with. 

The \emph{L\'evy Uniqueness Theorem} asserts that if $\mu_1$ and $\mu_2$ have the same characteristic function, then $\mu_1 = \mu_2$. In other words, a measure $\mu$ can be ``reconstructed'' from its characteristic function, and so the characteristic function of a measure determines the measure uniquely.  Let $(\mu_n)$ be a sequence of distributions, where each $\mu_n$ has characteristic function $\ph_n$, and let $\mu$ be a distribution with characteristic function $\ph$. The \emph{L\'evy Continuity Theorem} states that $\mu_n$ converges to $\mu$ weakly if and only if $\ph_n(t)$ converges to $\ph(t)$ for every $t$.

Remember that the CLT asserts that if $(X_n)$ is a sequence of random variable satisfying certain hypotheses, and, for each $n$, $\mu_n$ is a certain distribution defined in terms of $X_1, \ldots, X_n$, then $\mu_n$ converges weakly to the standard normal distribution. The L\'evy Continuity Theorem provides a straightforward strategy to prove the theorem: if we let $\ph_n$ denote the characteristic function of $\mu_n$ for each $n$, we need only show that $\ph_n$ approaches the characteristic function of the standard normal distribution pointwise.

Implementing this strategy requires two key ingredients. First, one needs to know that the characteristic function of the standard normal distribution is $\ph(t) = e^{-t^2/2}$. Second, one needs to compute the characteristic functions of the distributions $\mu_n$, which are defined in terms of finite sums of the random variables $X_0, X_1, \ldots$, and show that they have the desired behavior. This is where the key property of characteristic functions comes into play.

Once all these components were in place, putting the pieces together was not hard. Given the continuity theorem, the characteristic function of the standard normal distribution, the result on the characteristic functions of sums of random variables, and suitable approximations to the complex exponential function, the proof of the Central Limit Theorem is quite short. In our formalization, it was only about 120 lines long. 

\section{Isabelle and its libraries}
\label{section:isabelle}

When we began our project, a good deal of infrastructure was already available in the Isabelle libraries, but we had to add to it substantially. The formalization thus provided a stress test, allowing us to fill in gaps in the library and ensure its practical efficacy. In this section, we will describe those features of Isabelle and its libraries that were most relevant to the formalization, as well as indicate some of our contributions to the latter.

\subsection{The Isabelle proof assistant}

\todo{
Boilerplate, with references.

Simple type theory.

Axiomatic type classes. In our formalization, we used them for the algebraic hierarchy, analysis (normed spaces, Euclidean spaces), topological notions.

Locales. In our formalization, we used them for measures.

Isar, tactic proofs. We aimed for declarative proofs.

Automation: auto, simp. Sledgehammer and Metis (how much did we use these?). SMT?
}

\subsection{Topology and Limits}

When we began the project, we had at our disposal Isabelle's extensive library for topological spaces, including properties of open and closed sets, limits, compactness, continuity, and so on. The library is well-described in \cite{hoelzl:et:al:13}. Topological notions interact with measure theoretic notions in various ways. For example, a real distribution is a measure on the real numbers that measures the \emph{Borel sets}, the smallest $\sigma$-algebra containing the opens. Continuous functions are therefore measurable. We will see below that topological notions come into the statements of many measure-theoretic theorems, including points of continuity of a function, or the boundary of a set. Proving Skorohod's theorem, described below, required showing that the set of points of continuity of an arbitrary function from reals to reals is Borel; this is done in a four-line footnote in Billingsley (\cite[page 334]{billingsley:95}), and amounts to characterizing the set if discontinuities is a union of an intersection of open sets.

Conventional reasoning about limits was ubiquitous in our formalization. Everyday mathematics requires one to deal with expressions such as the following:
\begin{itemize}
 \item $\lim_{x \to a} f(x) = b$
 \item $\lim_{n \to \infty} a_n = a$
 \item $\lim_{x \to \infty} f(x) = b$
 \item $\lim_{x \to a^-} f(x) = b$
 \item $\lim_{x \to a} f(x) = \infty$
\end{itemize}
Here, the source and target spaces can be any topological space, including metric spaces or the natural numbers with the order topology. One can consider limits as $x$ approaches a value $a$, or $\infty$, or $-\infty$. One can also restrict consider the limit as $x$ approaches $a$ within a set $s$; saying $x$ approaches $a$ from the left (where $x$ and $a$ are real-valued, for example) is equivalent to saying that $x$ approaches $a$ on the interval $(-\infty, a)$. There is a similar range of variations on the output: $f(x)$ can approach a value, $b$, or $\infty$, or $-\infty$; and it can approach the value from the left, or from the right, or on any subset of the range of $f$. Not only does this threaten a combinatorial explosion of definitions, but also redundancy. For example, assuming $f(x)$ and $g(x)$ converge as $x$ approaches $a$, we have the identity $\lim_{x \to a} f(x) + g(x) = \lim_{x \to a} f(x) + \lim_{x \to a} g(x)$, but this also holds under all the variations of convergence in the source.

To handle the many instances of convergence that arose in the formalization, we used Isabelle's elegant library for dealing with limits via filters, as described in \cite{hoelzl:et:al:13}. The idea is that when dealing with any notion of limit, the relevant notions of convergence in the source and the target can be represented by \emph{filters}. A {\em filter} over $X$ is a nonempty set $\mathcal F \subseteq \mathcal P(X)$ such that if $A \subseteq B$ and $A \in \mathcal F$, then $B \in \mathcal F$, and if $A, B \in \mathcal F$, then $A \cap B \in \mathcal F$. The general notion of limit in Isabelle, {\tt filterlim f F1 F2}, says, roughly, that the function $f$ ``converges'' in the sense of $F_2$, as the input converges in the sense of $F_1$. By specializing $F_1$ and $F_2$ appropriately, we obtain all the variations described in the last paragraph, and more. In addition, theorems can be proved at the appropriate level of generality. For example, we have:
\begin{quote}
 \todo{Insert: Isabelle formulation of addition under limits.}
\end{quote}
This avoids the need to formalize endless variations of the same theorem. Details can be found in \cite{hoelzl:et:al:13}.

\todo{Talk about automation: introduction rules for open / closed sets, introduction rules for continuity.}

\subsection{Measure theory and probability}

\todo{
Overview, refer to ``Three Chapters of Measure Theory,'' \cite{hoelzl:heller:11}. Can also use text from Luke's thesis.

Measures, AE quantifier, and rules. Often used to ``ignore'' the behavior of functions on a set of measure zero, or a countable set in particular. 

Borel measures. Lebesgue measure. Product measure.}

Our initial formalization of the Central Limit Theorem relied on the theory of Lebesgue integration, described in \cite{hoelzl:heller:11}. This provides a notion of integration for suitable functions $f : X \to \RR$, where $X$ is any space on which a measure is defined. After we completed the proof, however, the second author, H\"olzl, generalized the construction to the \emph{Bochner integral}, which provides a theory of integration for functions $f : X \to B$, where now $B$ is any Banach space. In particular, $B$ can be any of the spaces $\RR^n$, or the complex numbers, $\CC$. Our formalization made extensive use of integration of functions from $\RR$ to $\CC$, as described in the next section.

\todo{Discuss the details of the Bochner integral.}

As with the Lebesgue integral, we obtain a version of the dominated convergence theorem.
\begin{quote}
 \todo{Insert a formal statement of this.}
\end{quote}
We also obtain the monotone convergence theorem, for sequences of functions taking values in the real numbers. We also obtain Fubini's theorem. These are staples of the theory of integration, and were used throughout our formalization.

If $f : X \to B$ is any measurable function on a space $X$ with measure $\mu$, and $S$ is any measurable set, one can define $\int_S f \, d\mu = \int f \chi_S \, d\mu$, where $\chi_S$ is the \emph{characteristic function} of $S$, also called the \emph{indicator function}. Rather than introduce a new definition, we took notation for integration over sets to be an abbreviation for the definition in terms of indicators. But because reasoning about integrals over sets is so fundamental, we found it helpful to develop a small library to support it. For example, the following is a consequence of the dominated convergence theorem:
\begin{quote}
\todo{Insert lebesgue-integral-countable-add from Set-Integral.thy}
\end{quote}

Any measure gives rise to a notion of integral. Conversely, with the integral in hand, functions can be used to construct new integrals.

\todo{Talk about densities, and the push forward measure.}

This is particularly useful for defining specific measures, or \emph{distributions}. 

\todo{For example, the uniform distribution is defined as follows\ldots The normal distribution is defined as follows\ldots}

\todo{Independence}

\todo{Automation: tools for proving measurability.}

In the Isabelle 2016 distribution, the new construction of the Bochner integral is in the file \verb=Bochner_Integration=. 

\subsection{Real analysis and complex-valued functions}
\label{subsect:real:analysis:section}

When we began the project, we also had at our disposal Isabelle's extensive library for real multivariate analysis, which is again well described in \cite{hoelzl:et:al:13}. In Isabelle, the reals are instantiated as a complete ordered field, as well as a conditionally complete lattice, which means that nonempty bounded sets have sups and infs. The library also included definitions of transcendental functions like $\sin$, $\cos$, and $\fn{exp}$. In fact, the exponential function was defined generically for any Banach space, including the complex numbers. We also had the relation $e^{i x} = \cos x + i \sin x$ for real $x$.

Isabelle's general notion of the derivative is the \emph{Frechet derivative}, which makes sense for functions $f$ between any two Banach spaces. As with limits, the notion of Frechet derivative supports multiple modes of convergence; the inscription \verb=(f has_derivative D) F= means that the function $f$ has the bounded linear functional $D$ as derivative ``at'' the filter $F$. In practice, $F$ is usually the filter expressing that $D$ is the derivative at a point $x$, or that $D$ is the derivative at a point $x$ when we restrict attention to a subset $S$ of the source. The more familiar notion of the scalar derivative for functions from the reals to reals (or, more generally, from one normed field to another) is derived from the Frechet derivative as a special case. So is the notion of a vector derivative for functions from $\RR$ to $\RR^n$.

The characteristic function of a measure is a function from the reals, $\RR$, to the complex numbers, $\CC$. Reasoning about such functions is much easier than reasoning about the functions from $\CC$ to $\CC$ that arise in complex analysis. One can view a function $f : \RR \to \CC$ as essentially two functions from $\RR$ to $\RR$, $f^\fn{re}$ and $f^\fn{im}$, the first returning the real part and the second returning the imaginary part of the input. Integrals and derivatives of such functions can be understood in terms of the integrals and derivatives of these two parts.

In fact, for differentiation, we did not have to define a new notion of derivative: if we view the complex numbers as a two-dimensional real Banach space, the derivative we need is nothing more than the Frechet derivative. 

For integration, the story is more involved. Isabelle's library now has two forms of the integral. The multivariate analysis library generally relies on the gauge interval, which is defined for functions from $\RR^n$ to $\RR$. When we consider the reals, $\RR$, with the usual Lebesgue measure, the Bochner integral and the gauge integral agree on finite intervals, but otherwise the gauge integral is slightly more general: for a function $f : \RR \to \RR$ to be Bochner integrable, both the positive and negative parts of $f$ have to have a finite Bochner integral, whereas the gauge integral can accommodate some functions whose positive and negative parts cancel each out in a suitable fashion. Nonetheless, for the vast majority of applications, the Bochner integral is quite sufficient. Since our formalization required integration with general measures and spaces in addition to the usual integration over $\RR^n$, we used the Bochner integral throughout.

With Bochner integral, as with the Frechet derivative, integrating functions taking values in $\CC$ is no different from integrating functions taking values in $\RR$. Indeed, this was the primary motivation for generalizing from the Lebesgue integral to the Bochner integral.

\subsection{Calculus}
\label{subsection:calculus}

Our formalization required extensive use of calculus at an undergraduate level: integration by parts, Taylor series approximations, changes of variable, and so on. For example, the calculation of moments of the normal distribution required the following estimate on the complex exponential:
\[
 \left| e^{ix} - \sum_{k=0}^n \frac{(ix)^k}{k!} \right| \le \min\left(\frac{|x|^{n+1}}{(n+1)!}, \frac{2 x^n}{n!}]\right).
\]
We followed Billingsley~\cite[Section 26]{billingsley:95} in obtaining this using an inductive argument and integration by parts. Notice that this involves reasoning about functions from the real to complex numbers; as noted above, the relevant properties generally follow from the corresponding properties for real-valued functions, upon splitting functions to the real and imaginary parts. In addition, we have the general inequality, $\| \int_A f \; d\mu \| \le \int_A \| f \| d\mu$, which allows us to bound the modulus of a complex interval by bounding the real-valued integral of the norm. This is an instance of a more general fact about the Bochner integral:
\begin{quote}
\todo{Insert integral-norm-bound}
\end{quote}

Textbook results from calculus involve integrals $\int_a^b f(x) \; dx$ over the interval $(a,b)$. These can be viewed simply as an ordinary integral over the set $(a,b)$, with the following two caveats:
\begin{itemize}
 \item Textbooks allow $a$ to be $-\infty$ and $b$ to be $\infty$, which is to say, $a$ and $b$ should be taken to be extended real numbers.
 \item It is convenient to adopt the convention that if $b < a$, then $\int_a^b f(x) \; dx = -\int_b^a f(x) dx$.
\end{itemize}
We thus defined a notion of ``interval integral'' along these lines, together with supporting notation. We could then state the first fundamental theorem of calculus in the following form, for finite intervals:
\begin{quote}
\todo{Insert interval-integral-FTC-finite}
\end{quote}
The following version, for arbitrary intervals, makes sense when the limits are infinite:
\begin{quote}
\todo{Insert interval-integral-FTC-integrable}
\end{quote}
Similarly, we could state the second fundamental theorem of calculus, where the variable bound to the integral can be before or after the fixed endpoint:
\begin{quote}
\todo{Insert interval-integral-FTC2}
\end{quote}
The use of such an integral was a mixed blessing. It simplified many of our theorems and proofs, at the expense of introducing yet another notion of integral, which required another library of supporting facts, as well as, at times, translations to and from the notion of integral over a set.

Many textbook integration arguments require a change of variable, sometimes known as ``integration by substitution.'' We proved that if a function $g$ from $\RR$ to $\RR$ has a continuous derivative (and hence is continuous itself) on a closed interval $[a,b]$, and $f$ is continuous on the image of $[a, b]$ under $g$, then $\int_a^b f(g(x)) g'(x) \; dx = \int_{g(a)^g(b)} f(x) \; dx$:
\begin{quote}
 \todo{Insert interval-integral-substitution-finite}
\end{quote}
Manuel Eberl later generalized this to arbitrary Borel measurable functions $f$, but with the added hypothesis that $g'$ is nonnegative on $[a, b]$. However, we also needed a version of this for intervals with potentially infinite endpoints. This requires using either the monotone convergence theorem or the dominated convergence theorem, to pass from finite interval approximations to the full interval. In fact, we proved two versions. The following one requires showing independently that both $f(x)$ and $f(g(x)) * g'(x)$ are integrable over the relevant intervals:
\begin{quote}
 \todo{Insert interval-integral-substitution-integrable}
\end{quote}
Another version assumes instead that $f$ is nonnegative, and concludes that $f$ is therefore integrable.

As an example where various among these components came together, consider the \emph{sine integral function}. The function $\sin x / x$ is undefined at $0$, but the function can be made continuous at $0$ by giving it the value $1$ there. The resulting function is called $\sinc$. The sine integral function is defined to be the indefinite integral of the $\sinc$ function, starting at $0$:
\[ 
\fn{Si}(t) = \int_0^t \sinc x \, dx. 
\]
The proof of the L\'evy inversion formula uses the fact that 
\[ 
\lim_{t \rightarrow \infty} \fn{Si}(t) = \frac{\pi}{2}.
\]
A calculus textbook proof (sketched in Billingsley \cite[Example 18.4]{billingsley:95} runs as follows. By the fundamental theorem of calculus, we can be verify that
\[ 
\int_0^t e^{-ux} \sin x \, dx = \frac{1}{1+u^2}[1 - e^{-ut}(u \sin t + \cos t)] 
\]
by taking the derivative of both sides.  Taking $t \rightarrow \infty$, we see that
\[ 
\int_0^t \left( \int_0^\infty |e^{-ux} \sin x| \, du\right) \, dx = \int_0^t x^{-1} |\sin x| \, dx \le t,
\]
so Fubini's theorem may be used in the integration of $e^{-ux} \sin x$ over $(0,t) \times (0, \infty)$:
\begin{align*}
\int_0^t \frac{\sin x}{x} \, dx &= \int_0^t \sin x \left(\int_0^\infty e^{-ux} \, du\right) \, dx \\
                                &= \int_0^\infty \left(\int_0^t e^{-ux} \sin x \, dx\right) \, du \\
                                &= \int_0^\infty \frac{du}{1+u^2} - \int_0^\infty \frac{e^{-ut}}{1+u^2} (u \sin t + \cos t) \, du.
\end{align*}
Substituting $u = \tan x$ in the first term yields
\[
 \int_0^\infty \frac{du}{1+u^2} = \int_0^{\pi/2} \frac{1}{1 + \tan^2 x} (1 + \tan^2 x) \, dx = \pi/2,
\]
and the change of variable $v = ut$ can be used to see that the second integral in the final result of the above calculation converges to $0$ as $t \rightarrow \infty$. Hence
\[ 
\lim_{t \rightarrow \infty} \fn{Si}(t) = \lim_{t \rightarrow \infty} \int_0^t \frac{\sin x}{x} \, dx = \frac{\pi}{2},
\]
as required. 

Proving this result required a tremendous amount of formal machinery: not only suitable forms of substitution, but also Fubini's theorem, the fundamental theorem of calculus, integration by parts, integral comparisons, properties of limits, and properties of the tangent function. It also required a fair amount of work, establishing that the relevant functions were continuous, integrable, and so on. It was somewhat demoralizing that a small calculus exercise required so much effort, but it is a good illustration of the infrastructure that is needed to carry out the kinds of calculus computations that come up routinely in engineering, modeling, and the sciences. A similar calculus exercise involved computing the moments of the normal distribution, as described in Section~\ref{subsection:characteristic}.

In the Isabelle 2016 distribution, the fundamental theorem and substitution theorems cited are in the theory \verb=Interval_Integral=, and Eberl's generalization is in the theory \verb=Lebesgue_Integral_Substitution=. The calculation concerning $\fn{Si}$ is in the file \verb=Sinc_Integral=.

\subsection{Distribution functions and the Lebesgue-Stieltjes measure}

Every measure on $\RR$ gives rise to the real valued function which, at each input $x$, returns the amount of ``mass'' below that argument:

\begin{definition}
Let $\mu$ be a finite measure on $\RR$. The \emph{cumulative distribution function} $F_\mu$ is defined by $F_\mu(x) = \mu (-\infty, x]$.
\end{definition}
The cumulative distribution function, or \emph{cf}, is sometimes also called, more simply, the \emph{distribution function}. In Isabelle, the definition is rendered as follows:

\begin{quote}
\begin{isabellebody}
\isacommand{definition}\isamarkupfalse%
\isanewline
\ \ cdf\ {\isacharcolon}{\isacharcolon}\ {\isachardoublequoteopen}real\ measure\ {\isasymRightarrow}\ real\ {\isasymRightarrow}\ real{\isachardoublequoteclose}\isanewline
\isakeyword{where}\isanewline
\ \ {\isachardoublequoteopen}cdf\ M\ {\isasymequiv}\ {\isasymlambda}x{\isachardot}\ measure\ M\ {\isacharbraceleft}{\isachardot}{\isachardot}x{\isacharbraceright}{\isachardoublequoteclose}
\end{isabellebody}
\end{quote}

It is not hard to see that the distribution function $F_\mu$ of a finite measure $\mu$ is nondecreasing and right-continuous, and satisfies $\lim_{x \rightarrow -\infty} F_\mu(x) = 0$ and \linebreak $\lim_{x \rightarrow \infty} F_\mu(x) = 1$.

\begin{quote}
\begin{isabellebody}
\isacommand{lemma}\isamarkupfalse%
\ cdf{\isacharunderscore}nondecreasing{\isacharcolon}\ {\isachardoublequoteopen}{\isacharparenleft}{\isasymforall}x\ y{\isachardot}\ x\ {\isasymle}\ y\ {\isasymlongrightarrow}\ cdf\ M\ x\ {\isasymle}\ cdf\ M\ y{\isacharparenright}{\isachardoublequoteclose}\isanewline

\isacommand{lemma}\isamarkupfalse%
\ cdf{\isacharunderscore}is{\isacharunderscore}right{\isacharunderscore}cont{\isacharcolon}\ {\isachardoublequoteopen}continuous\ {\isacharparenleft}at{\isacharunderscore}right\ a{\isacharparenright}\ {\isacharparenleft}cdf\ M{\isacharparenright}{\isachardoublequoteclose}\isanewline

\isacommand{lemma}\isamarkupfalse%
\ cdf{\isacharunderscore}lim{\isacharunderscore}at{\isacharunderscore}bot{\isacharcolon}\ {\isachardoublequoteopen}{\isacharparenleft}cdf\ M\ {\isacharminus}{\isacharminus}{\isacharminus}{\isachargreater}\ {\isadigit{0}}{\isacharparenright}\ at{\isacharunderscore}bot{\isachardoublequoteclose}\isanewline

\isacommand{lemma}\isamarkupfalse%
\ cdf{\isacharunderscore}lim{\isacharunderscore}at{\isacharunderscore}top{\isacharunderscore}prob{\isacharcolon}\ {\isachardoublequoteopen}{\isacharparenleft}cdf\ M\ {\isacharminus}{\isacharminus}{\isacharminus}{\isachargreater}\ {\isadigit{1}}{\isacharparenright}\ at{\isacharunderscore}top{\isachardoublequoteclose}
\end{isabellebody}
\end{quote}

Conversely, it turns out that any function with these properties is the distribution of a Borel probability measure on $\RR$. The requisite measure $\mu$ is constructed by defining $\mu (a,b] = F(b) - F(a)$ and extending this to the Borel $\sigma$-algebra using the Carath\'eodory extension theorem.
\begin{quote}
\begin{isabellebody}
\isacommand{lemma}\isamarkupfalse%
\ real{\isacharunderscore}distribution{\isacharunderscore}interval{\isacharunderscore}measure{\isacharcolon}\isanewline
\ \ \isakeyword{fixes}\ F\ {\isacharcolon}{\isacharcolon}\ {\isachardoublequoteopen}real\ {\isasymRightarrow}\ real{\isachardoublequoteclose}\isanewline
\ \ \isakeyword{assumes}\ nondecF\ {\isacharcolon}\ {\isachardoublequoteopen}{\isasymAnd}\ x\ y{\isachardot}\ x\ {\isasymle}\ y\ {\isasymLongrightarrow}\ F\ x\ {\isasymle}\ F\ y{\isachardoublequoteclose}\ \isakeyword{and}\isanewline
\ \ \ \ right{\isacharunderscore}cont{\isacharunderscore}F\ {\isacharcolon}\ {\isachardoublequoteopen}{\isasymAnd}a{\isachardot}\ continuous\ {\isacharparenleft}at{\isacharunderscore}right\ a{\isacharparenright}\ F{\isachardoublequoteclose}\ \isakeyword{and}\ \isanewline
\ \ \ \ lim{\isacharunderscore}F{\isacharunderscore}at{\isacharunderscore}bot\ {\isacharcolon}\ {\isachardoublequoteopen}{\isacharparenleft}F\ {\isacharminus}{\isacharminus}{\isacharminus}{\isachargreater}\ {\isadigit{0}}{\isacharparenright}\ at{\isacharunderscore}bot{\isachardoublequoteclose}\ \isakeyword{and}\isanewline
\ \ \ \ lim{\isacharunderscore}F{\isacharunderscore}at{\isacharunderscore}top\ {\isacharcolon}\ {\isachardoublequoteopen}{\isacharparenleft}F\ {\isacharminus}{\isacharminus}{\isacharminus}{\isachargreater}\ {\isadigit{1}}{\isacharparenright}\ at{\isacharunderscore}top{\isachardoublequoteclose}\isanewline
\ \ \isakeyword{shows}\ {\isachardoublequoteopen}real{\isacharunderscore}distribution\ {\isacharparenleft}interval{\isacharunderscore}measure\ F{\isacharparenright}{\isachardoublequoteclose}
\end{isabellebody}
\end{quote}
Here \texttt{real\_distribution} is the name of the locale for Borel probability measures, and \texttt{interval\_measure} is the function that generates a measure from a nondecreasing, right-continuous function. To apply the Caratheodory extension theorem, the key property that needed to verified that is half-open interval $(a, b]$ is written as a disjoint union of countably many intervals $(a_i, b_i]$, then $b - a = \sum_i (b_i - a_i)$. This is tricker than it sounds. For example, the interval $(0, 1]$ can be written as a countable union of intervals $(1/2^{i+1}, 1/2^i]$, and any one of \emph{those} intervals could similarly be replaced by a countable union. It is not too hard to show that the infinite sum $\sum_i (b_i - a_i]$ is bounded by $b - a$. In the other direction, one picks a small $\epsilon$, enlarges each interval $(a_i, b_i]$ to a slightly larger interval $(a_i - \varepsilon/2^i, b_i + \varepsilon / 2^i)$, argues that the union of the enlargements covers the closed interval $[a, b]$, and then applies compactness.

The measure associated to a right-continuous, nondecreasing function in this way is called the \emph{Lebesgue-Stieltjes measure}. The use of the word ``the'' is justified by the fact that the association is unique, in the sense that if two real distributions have the same cumulative distribution function, then they are equal: 
\begin{quote}
\begin{isabellebody}
\isacommand{lemma}\isamarkupfalse%
\ cdf{\isacharunderscore}unique{\isacharcolon}\isanewline
\ \ \isakeyword{fixes}\ M{\isadigit{1}}\ M{\isadigit{2}}\isanewline
\ \ \isakeyword{assumes}\ {\isachardoublequoteopen}real{\isacharunderscore}distribution\ M{\isadigit{1}}{\isachardoublequoteclose}\ \isakeyword{and}\ {\isachardoublequoteopen}real{\isacharunderscore}distribution\ M{\isadigit{2}}{\isachardoublequoteclose}\isanewline
\ \ \isakeyword{assumes}\ {\isachardoublequoteopen}cdf\ M{\isadigit{1}}\ {\isacharequal}\ cdf\ M{\isadigit{2}}{\isachardoublequoteclose}\isanewline
\ \ \isakeyword{shows}\ {\isachardoublequoteopen}M{\isadigit{1}}\ {\isacharequal}\ M{\isadigit{2}}{\isachardoublequoteclose}
\end{isabellebody}
\end{quote}
Thus one can pass freely between talk of measures on $\RR$ and their distribution functions, a key fact in the proof of the CLT. When the function $F(x)$ is the identity function, we obtain Lebesgue measure, and, in fact, this now serves as the definition of Lebesgue measure in the Isabelle library.

In the Isabelle 2016 distribution, the construction of Lebesgue measure on the reals as a Lebesgue-Stieltjes is in the theory \verb=Lebesgue_Measure=, and the correspondence between measures and their distribution functions is developed in \verb=Distributions=.

\section{The proof of the Central Limit Theorem}
\label{section:formal}

\subsection{Weak convergence}
\label{subsection:weak:convergence}

Recall from Section~\ref{subsection:background} that if $(\mu_n)$ is a sequence of real distributions and $\mu$ is a real distribution, then $\mu_n$ converge weakly $\mu$, written $\mu_n \Rightarrow \mu$, if $F_{\mu_n}(x)$ approaches $F_\mu(x)$ at each point $x$ where $F_\mu$ is continuous. In Isabelle, we expressed this as follows:

\begin{quote}
\begin{isabellebody}
\isacommand{definition}\isamarkupfalse%
\isanewline
\ \ weak{\isacharunderscore}conv\ {\isacharcolon}{\isacharcolon}\ {\isachardoublequoteopen}{\isacharparenleft}nat\ {\isasymRightarrow}\ {\isacharparenleft}real\ {\isasymRightarrow}\ real{\isacharparenright}{\isacharparenright}\ {\isasymRightarrow}\ {\isacharparenleft}real\ {\isasymRightarrow}\ real{\isacharparenright}\ {\isasymRightarrow}\ bool{\isachardoublequoteclose}\isanewline
\isakeyword{where}\isanewline
\ \ {\isachardoublequoteopen}weak{\isacharunderscore}conv\ F{\isacharunderscore}seq\ F\ {\isasymequiv}\ {\isasymforall}x{\isachardot}\ isCont\ F\ x\ {\isasymlongrightarrow}\ {\isacharparenleft}{\isasymlambda}n{\isachardot}\ F{\isacharunderscore}seq\ n\ x{\isacharparenright}\ {\isacharminus}{\isacharminus}{\isacharminus}{\isacharminus}{\isachargreater}\ F\ x{\isachardoublequoteclose}\isanewline\isanewline
\isacommand{definition}\isamarkupfalse%
\isanewline
\ \ weak{\isacharunderscore}conv{\isacharunderscore}m\ {\isacharcolon}{\isacharcolon}\ {\isachardoublequoteopen}{\isacharparenleft}nat\ {\isasymRightarrow}\ real\ measure{\isacharparenright}\ {\isasymRightarrow}\ real\ measure\ {\isasymRightarrow}\ bool{\isachardoublequoteclose}\isanewline
\isakeyword{where}\isanewline
\ \ {\isachardoublequoteopen}weak{\isacharunderscore}conv{\isacharunderscore}m\ M{\isacharunderscore}seq\ M\ {\isasymequiv}\ weak{\isacharunderscore}conv\ {\isacharparenleft}{\isasymlambda}n{\isachardot}\ cdf\ {\isacharparenleft}M{\isacharunderscore}seq\ n{\isacharparenright}{\isacharparenright}\ {\isacharparenleft}cdf\ M{\isacharparenright}{\isachardoublequoteclose}
\end{isabellebody}
\end{quote}
In other words, a sequence of functions $(F_n)_{n \in \NN}$ converges weakly to $F$ if $(F_n(x))_{n \in \NN}$ converges to $F(x)$ for each point $x$ where $F$ is continuous. The sequence of measures $(\mu_n)$ converges weakly to $(\mu)$ is the corresponding cumulative distribution functions converge weakly.

That the notion of weak convergence is robust is supported by the fact that there are a number of equivalent characterizations. The following theorem is sometimes known as the \emph{Portmanteau Theorem}:
\begin{theorem}
The following are equivalent:
\begin{enumerate}
 \item $\mu_n \Rightarrow \mu$
 \item $\int f \; d\mu_n$ approaches $\int f \; d\mu$ for every bounded function $f$ at is continuous almost everywhere.
 \item $\int f \; d\mu_n$ approaches $\int f \; d\mu$ for every bounded, continuous function $f$
 \item If $A$ is any Borel set, $\partial A$ denotes the topological boundary of $A$, and $\mu(\partial A) = 0$, then $\mu_n(A)$ approaches $\mu_n(A)$. 
\end{enumerate}
\end{theorem}
The theorem is interesting in that it combines measure-theoretic notions (measures and the integral) with topological notions (continuity and topological boundaries). The proof from 1 to 2 uses Skorohod's theorem, which states that if $(\mu_n)$ is a sequence of real distributions on the reals that converges to a real distribution $\mu$, there is a sequence $(Y_n)$ of random variables and another random variable, on a common probability space, such that each $Y_n$ has distribution $\mu_n$, $Y$ has distribution $\mu$, and $Y_n$ converges to $Y$ pointwise. In other words, $(\mu_n)$ and $\mu$ can be represented in a particularly nice way. 
\begin{quote}
\begin{isabellebody}
\isacommand{theorem}\isamarkupfalse%
\ Skorohod{\isacharcolon}\isanewline
\ \ \isakeyword{fixes}\ \isanewline
\ \ \ \ {\isasymmu}\ {\isacharcolon}{\isacharcolon}\ {\isachardoublequoteopen}nat\ {\isasymRightarrow}\ real\ measure{\isachardoublequoteclose}\ \isakeyword{and}\isanewline
\ \ \ \ M\ {\isacharcolon}{\isacharcolon}\ {\isachardoublequoteopen}real\ measure{\isachardoublequoteclose}\isanewline
\ \ \isakeyword{assumes}\ \isanewline
\ \ \ \ {\isachardoublequoteopen}{\isasymAnd}n{\isachardot}\ real{\isacharunderscore}distribution\ {\isacharparenleft}{\isasymmu}\ n{\isacharparenright}{\isachardoublequoteclose}\ \isakeyword{and}\ \isanewline
\ \ \ \ {\isachardoublequoteopen}real{\isacharunderscore}distribution\ M{\isachardoublequoteclose}\ \isakeyword{and}\ \isanewline
\ \ \ \ {\isachardoublequoteopen}weak{\isacharunderscore}conv{\isacharunderscore}m\ {\isasymmu}\ M{\isachardoublequoteclose}\isanewline
\ \ \isakeyword{shows}\ {\isachardoublequoteopen}{\isasymexists}\ {\isacharparenleft}{\isasymOmega}\ {\isacharcolon}{\isacharcolon}\ real\ measure{\isacharparenright}\ {\isacharparenleft}Y{\isacharunderscore}seq\ {\isacharcolon}{\isacharcolon}\ nat\ {\isasymRightarrow}\ real\ {\isasymRightarrow}\ real{\isacharparenright}\ {\isacharparenleft}Y\ {\isacharcolon}{\isacharcolon}\ real\ {\isasymRightarrow}\ real{\isacharparenright}{\isachardot}\ \isanewline
\ \ \ \ prob{\isacharunderscore}space\ {\isasymOmega}\ {\isasymand}\isanewline
\ \ \ \ {\isacharparenleft}{\isasymforall}n{\isachardot}\ Y{\isacharunderscore}seq\ n\ {\isasymin}\ measurable\ {\isasymOmega}\ borel{\isacharparenright}\ {\isasymand}\isanewline
\ \ \ \ {\isacharparenleft}{\isasymforall}n{\isachardot}\ distr\ {\isasymOmega}\ borel\ {\isacharparenleft}Y{\isacharunderscore}seq\ n{\isacharparenright}\ {\isacharequal}\ {\isasymmu}\ n{\isacharparenright}\ {\isasymand}\isanewline
\ \ \ \ Y\ {\isasymin}\ measurable\ {\isasymOmega}\ lborel\ {\isasymand}\isanewline
\ \ \ \ distr\ {\isasymOmega}\ borel\ Y\ {\isacharequal}\ M\ {\isasymand}\isanewline
\ \ \ \ {\isacharparenleft}{\isasymforall}x\ {\isasymin}\ space\ {\isasymOmega}{\isachardot}\ {\isacharparenleft}{\isasymlambda}n{\isachardot}\ Y{\isacharunderscore}seq\ n\ x{\isacharparenright}\ {\isacharminus}{\isacharminus}{\isacharminus}{\isacharminus}{\isachargreater}\ Y\ x{\isacharparenright}{\isachardoublequoteclose}
\end{isabellebody}
\end{quote}
This proof presented a number of technical challenges for formalization. For one, we needed to choose a continuity point of an arbitrary probability measure in an arbitrary open interval, that is, a real number $x$ in an open interval $I$ such that the measure of $\{x\}$ is zero. To that end, we showed that the number of atoms of a measure (that is, points $x$ such that $\{x\}$ has strictly positive measure) is countable:
\begin{quote}
\begin{isabellebody}
\isacommand{lemma}\isamarkupfalse%
\ countable{\isacharunderscore}atoms{\isacharcolon}\ {\isachardoublequoteopen}countable\ {\isacharbraceleft}x{\isachardot}\ measure\ M\ {\isacharbraceleft}x{\isacharbraceright}\ {\isachargreater}\ {\isadigit{0}}{\isacharbraceright}{\isachardoublequoteclose}
\end{isabellebody}
\end{quote}
The result then follows from the fact that any open interval in the reals is uncountable. 

The implication from 2 to 3 is immediate. Notice that 1 is equivalent to saying that for every point $x$ of continuity of the measure $\mu$, $\int \chi_{(-\infty,x]} \; d\mu_n$ approaches $\int \chi_{(-\infty,x]} \; d\mu$, where $\chi_{(-\infty,x]}$ is the characteristic function of the interval $(-\infty,x]$. The implication from 3 to 1 is obtained by approximating this characteristic function by continuous step functions whenever $x$ is a point of continuity. The implication from 4 to 1 is easy, noticing that $(-\infty,x]$ is a set of the specified type, whenever $x$ is a point of continuity of $\mu$. To close the chain, it is enough to prove that 2 implies 4. This implication is also not hard, once we show that the characteristic function $\chi_A$ is bounded and continuous at any point not on the boundary.

The result discussed in this section can be found in the theory \verb=Weak_Convergence=.

\subsection{Characteristic functions}
\label{subsection:characteristic}

Recall the the \emph{characteristic function} $\ph$ of a probability measure $\mu$ on the real line is defined by
\[
\ph(t) = \int_{-\infty}^{\infty} e^{itx} \mu(dx).
\]
If $X$ is a random variable, the characteristic function of $X$ is defined to be the characteristic function of its distribution.
In our formalization, these characteristic function of a measure is defined as follows:
\begin{quote}
 \todo{Insert.}
\end{quote}
The characteristic function of a random variable \texttt{X} defined on a measure space \texttt{M} is then written \texttt{char (distr M borel X)}, since \texttt{distr M borel X} denotes the distribution of \texttt{X} with respect to the usual Borel measure on the real numbers.

The characteristic function $\ph$ of a measure is continuous, and satisfied $\ph(0) = 1$ and $\ph(t) \leq 1$ for every $t$:
\begin{quote}
 \todo{Insert isCont-chair, char-zero, cmod-char-le-1}
\end{quote}
The key property of characteristic functions is this: if $X_1$ and $X_2$ are independent random variables defined with respect to a measure, $M$, the characteristic function of $X_1 + X_2$ is the product of the individual characteristic functions. Because we used the Bochner integral, which allows us to integrate complex-valued functions directly, our final proof of this fact is even simpler than the one in Billingsley \cite{billingsley:95}. The calculation runs as follows:
\begin{align*}
\ph_{X_1 + X_2}(t) &= \int e^{i t (X_1 + X_2)} \, dM  \\
                   &= \int e^{i t X_1} e^{i t X_2} \, dM \\
                   &= \left(\int e^{i t X_1} \, dM\right) \left(\int e^{i t X_2} \, dM\right) \\
                   &= \ph_{X_1}(t) \ph_{X_2}(t).
\end{align*}
Here, $X_1$ and $X_2$ are really functions over the underlying measure space, and the third equation follows from the independence of $X_1$ and $X_2$. We reproduce our formal proof, as well as the generalization to arbitrary finite sums, in full:
\begin{quote}
\todo{Insert char-distr-sum and char-distr-setsum, in full.}
\end{quote}
We also needed an explicit approximations to the characteristic functions of a random variable, obtained using the calculation described at the beginning of Section~\ref{subsection:calculus}. One formulation is as follows:
\begin{quote}
 \todo{Insert char-approx3'}
\end{quote}

Finally, we needed to compute the characteristic function $\ph$ of the standard normal distribution, and show $\ph(t) = e^{-t^2/2}$. Establishing this fact took more work than we thought it would. Many textbook proofs of this invoke facts from complex analysis that were unavailable to us. Billingsley \cite[page 344]{billingsley:95} sketches an elementary proof, which required calculating the moments and absolute moments of the standard normal distribution. This is where the calculations of $\int_{-\infty}^\infty x^k e^{-x^2 / t} \; dx$ mentioned in Section~\ref{subsection:calculus}, were needed: specifically, we have for even $k$, 
\begin{quote}
\todo{Insert std-normal-moment-even in Distributions}
\end{quote}
and for odd $k$,
\begin{quote}
\todo{Insert std-normal-moment-odd}
\end{quote}
A prior calculation by Sudeep Kanav covered the cases $k = 0, 1$, which provide the base cases for an inductive proof. Filling in the details involved carrying out careful computations with integrals and power series approximations to $e^x$.

In the Isabelle 2016 distribution, characteristic functions are defined in the theory \verb=Characteristic_Functions=, and the properties cited above are proved there. The calculation of the moments of the normal distribution is found in \verb=Distributions=.

\subsection{L\'evy inversion and uniqueness}

In Fourier analysis, an ``inversion theorem'' says that, under suitable hypotheses and in a suitable sense, a function can be recovered from its Fourier transform. Along those lines, the L\'evy Inversion and Uniqueness Theorems say that a measure can be recovered from its characteristic function. 

More precisely, the L\'evy Inversion Theorem states the following:
\begin{theorem}
Let $\mu$ be a probability measure, and $\phi$ be the characteristic function of $\mu$. If $a$ and $b$ are continuity points of $\mu$ and $a < b$, then
\[ 
\mu (a,b] = \lim_{T \rightarrow \infty} \frac{1}{2\pi} \int_{-T}^T \frac{e^{-ita} - e^{-itb}}{it} \ph(t) \, dt. 
\]
\end{theorem}

The proof is a long and subtle calculation. Let $I(T)$ denote the expression after the limit. Expanding the definition of $\ph(t)$ and appealing to Fubini's theorem to switch the order of the two integrals, we obtain
\[
I(T) = \frac{1}{2\pi} \int_{-\infty}^\infty \int_{-T}^T \frac{e^{it(x-a)} - e^{it(x-b)}}{it} \, dt \, \mu(dx). 
\]
The idea is that as $T$ approaches $\infty$, the inner integral approaches a step function which jumps from $0$ to $1$ at $a$ and then back down to $0$ at $b$. This is shown by expanding the complex exponential in terms of $\sin$ and $\cos$, using properties of the sine integral, and messing around with integrals and limits.

It is not hard to show that fixing the values of a measure on intervals $(a, b]$ as above is enough to determine the measure on all Borel sets. Thus the Inversion Theorem has the following Uniqueness Theorem as an important corollary:
\begin{theorem}
If $\mu_1$ and $\mu_2$ are probability measures and $\ph_{\mu_1} = \ph_{\mu_2}$, then $\mu_1 = \mu_2$. 
\end{theorem}
In our formalization, this is expressed simply as follows:
\begin{quote}
 \todo{Insert Levy uniqueness}
\end{quote}

\subsection{The L\'evy Continuity Theorem}

Let $(\mu_n)$ be a sequence of distributions, where each $\mu_n$ has characteristic function $\ph_n$, and let $\mu$ be a distribution with characteristic function $\ph$. The \emph{L\'evy Continuity Theorem} states that $\mu_n$ converges to $\mu$ weakly if and only if $\ph_n(t)$ converges to $\ph(t)$ for every $t$. In our formalization, it is expressed as follows:
\begin{quote}
 \todo{Insert.}
\end{quote}

Proving the ``only if'' direction is easy, using Portmanteau Theorem of Section~\ref{subsection:weak:convergence}, since $e^{itx}$ is bounded and continuous. In fact, in our formalization, it has a one-line proof:
\begin{quote}
 \todo{Insert levy-continuity1 and its one-line proof.}
\end{quote}
The other direction is a lot harder. The outline of the proof runs as follows:
\begin{enumerate}
 \item Use a compactness argument to show that every subsequence $(\mu_{n_k})$ of $(\mu_n)$ has a weakly convergent subsequence.
 \item Argue that if $(\mu_n)$ does not converge weakly to $\mu$, there is a subsequence $(\mu_{n_k})$ such that no subsequence of \emph{that} can converge weakly to $\mu$.
 \item By 1, $(\mu_{n_k})$ converges weakly to some measure, $\nu$. 
 \item By the ``only if'' direction, already proved, $\ph_{n_k}$ converges pointwise to the characteristic function of $\nu$.
 \item Since, by hypothesis, $\ph_n(t)$ converges to $\ph(t)$ for every $t$, the characteristic function of $\nu$ must be $\ph$.
 \item By the Uniqueness Theorem, this implies that $\nu = \mu$, contrary to the choice of $(\mu_{n_k})$ in 4.
\end{enumerate}

The necessary compactness principal is a consequence of the \emph{Helly Selection Theorem}, which we now describe. Although the proof of this theorem and its consequence take up only a page-and-a-half in Billingsley's textbook, these were among the most subtle components of our formalization. The theorem states the following:
\begin{theorem}
Let $(F_n)$ be a uniformly bounded sequence of nondecreasing, right continuous functions. Then there is a subsequence $(F_{n_k})$ and a nondecreasing, right-continuous function $F$ such that $\lim_k F_{n_k}(x) = F(x)$ at continuity points of $F$.  
\end{theorem}
In our formalization, this is expressed as follows:
\begin{quote}
  Insert.
\end{quote}
Note that the subsequence $(F_{n_k})$ is represented by a strictly increasing function $s : \NN \to \NN$ which returns, for each $k$, the value $n_k$. The proof involves a diagonalization argument: for each rational $r$, we thin the sequence to guarantee convergence at $r$, and then take a ``diagonal limit'' to construct the required subsequence and limit. To that end, we used a general framework for such diagonalization arguments, provided by Fabian Immler.

To describe the relevant corollary, we need to introduce a definition. A sequence $(\mu_n)$ of real measures is said to be \emph{tight} if for every $\varepsilon > 0$, there is a finite interval $(a, b]$ such that $\mu_n(a, b] > 1 - \varepsilon$ for all $n$. Roughly, a sequence of probability measures is tight if no mass ``escapes to infinity''; the sequence $(\mu_n)$, where $\mu_n$ is a unit mass at $n$, is an example of a sequence that is \emph{not} tight. Helly's theorem can be used to show that if $(\mu_n)$ is a tight sequence of measures, then for every subsequence $(\mu_{n_k})$ there is a further subsequence $(\mu_{n_{k(j)}})$ and a probability measure $\mu$ such that $(\mu_{n_{k(j)}})$ converges weakly $\mu$ as $j$ approaches infinity.
\begin{quote}
 \todo{Insert our formal version.}
\end{quote}

In the Isabelle 2016 distribution, the Helly Selection Theorem and its corollary are proved in \verb=Helly_Selection=, and the L\'evy Continuity Theorem is proved in \verb=Levy=.

\subsection{The Central Limit Theorem}

Proving the Central Limit Theorem is now just a matter of putting the pieces together. Let $(X_n)$ be a sequence of random variables all of which have the same distribution, $\mu$, and finite variance, $\sigma^2 > 0$. Without loss of generalizing, subtracting a common offset, we can assume that each $X_n$ has mean $0$. Let
\[
 S'_n = \sum_{i < n} X_i / \sqrt {n \sigma^2}
\]
be the normalized sums. Our goal is to show that the distributions of $S'_n$ converge weakly to the standard normal distribution. 

For each $n$, let $\ph_n$ be the characteristic function of $\Sigma'_n$. By the L\'evy continuity theorem, it suffices to show that $\ph_n$ approaches the characteristic function of the standard normal distribution points. In other words, we need to show that for every $t$, $\ph_n(t)$ approaches $e^{-t^2/2}$.

Since each $X_i$ has the same distribution, all the $X_i$'s have the same characteristic function; call it $\psi$. By the key property, the characteristic function of the sum $S'_n$ is the product of the characteristic functions of the components, so
\begin{align*}
 \ph_n(t) & = \prod_{j = 1}^n \int e^{itX_j} / \sqrt{n \sigma^2} \, d\mu \\
   & = \prod_{j = 1}^n \psi(t / \sqrt{n \sigma^2}) \\
   & = (\psi(t / \sqrt{n \sigma^2}))^n.
\end{align*}
Now some of the explicit calculations described in Section~\ref{subsection:calculus} can be used to show that $\psi(t)$ is well approximated by
\[
1 + it \int X \, d\mu + \frac{t^2}{2} \int X^2 d\mu, 
\]
which is equal to $1 - t^2\sigma^2 / 2$, since we are assuming $X$ has mean $0$ and variance $\sigma^2$. Plugging in $t / \sqrt{n \sigma^2}$ for $t$, we see that $\ph_n(t)$ is approximated by $(1 - \frac{t^2}{2n})^n$, which approaches $e^{-t^2/2}$ as $t$ approaches infinity.

The Central Limit Theorem is proved, of course, in the theory \verb=Central_Limit_Theorem=. The formal version of the proof we have just sketched is given in its entirety in the appendix. 

\section{Reflections}
\label{section:reflections}

\subsection{Dealing with partial functions}

\todo{
has-integral, etc.

Use example of integrable-monotone-convergence



Fubini, dominated convergence

Message: both representations are useful and needed, but you have to be careful
}

\subsection{Strategies for limit proofs}

\todo{
For example, use properties of ordering instead of epsilon delta
}

\subsection{Strategies for integrals}

\todo{
affine trick
}

\subsection{Alternatives}

\todo{
Stone-Weierstrass, complex analysis countour integrals
}

\subsection{Cleanup and length}

\todo{
Originally, combined, 13000

Now, things in this section, 2500

Infrastructure: interval integral, Bochner, set integral

Distributions

give line counts, esp.~for CLT, given infrastructure


cleanup:
\begin{itemize}
 \item moving things to libraries
 \item general refactoring, using general properties rather than fiddly proofs
 \item eliminating duplicated code
 \item choosing good names (esp.~for integrals), rather than ``billingsley 13.1''
\end{itemize}
}

\bibliographystyle{plain}
\bibliography{clt}

\section*{Appendix}

{
\scriptsize
\begin{isabellebody}
\isacommand{theorem}\isamarkupfalse%
\ {\isacharparenleft}\isakeyword{in}\ prob{\isacharunderscore}space{\isacharparenright}\ central{\isacharunderscore}limit{\isacharunderscore}theorem{\isacharcolon}\isanewline
\ \ \isakeyword{fixes}\ \isanewline
\ \ \ \ X\ {\isacharcolon}{\isacharcolon}\ {\isachardoublequoteopen}nat\ {\isasymRightarrow}\ {\isacharprime}a\ {\isasymRightarrow}\ real{\isachardoublequoteclose}\ \isakeyword{and}\isanewline
\ \ \ \ {\isasymmu}\ {\isacharcolon}{\isacharcolon}\ {\isachardoublequoteopen}real\ measure{\isachardoublequoteclose}\ \isakeyword{and}\isanewline
\ \ \ \ {\isasymsigma}\ {\isacharcolon}{\isacharcolon}\ real\ \isakeyword{and}\isanewline
\ \ \ \ S\ {\isacharcolon}{\isacharcolon}\ {\isachardoublequoteopen}nat\ {\isasymRightarrow}\ {\isacharprime}a\ {\isasymRightarrow}\ real{\isachardoublequoteclose}\isanewline
\ \ \isakeyword{assumes}\isanewline
\ \ \ \ X{\isacharunderscore}indep{\isacharcolon}\ {\isachardoublequoteopen}indep{\isacharunderscore}vars\ {\isacharparenleft}{\isasymlambda}i{\isachardot}\ borel{\isacharparenright}\ X\ UNIV{\isachardoublequoteclose}\ \isakeyword{and}\isanewline
\ \ \ \ X{\isacharunderscore}integrable{\isacharcolon}\ {\isachardoublequoteopen}{\isasymAnd}n{\isachardot}\ integrable\ M\ {\isacharparenleft}X\ n{\isacharparenright}{\isachardoublequoteclose}\ \isakeyword{and}\isanewline
\ \ \ \ X{\isacharunderscore}mean{\isacharunderscore}{\isadigit{0}}{\isacharcolon}\ {\isachardoublequoteopen}{\isasymAnd}n{\isachardot}\ expectation\ {\isacharparenleft}X\ n{\isacharparenright}\ {\isacharequal}\ {\isadigit{0}}{\isachardoublequoteclose}\ \isakeyword{and}\isanewline
\ \ \ \ {\isasymsigma}{\isacharunderscore}pos{\isacharcolon}\ {\isachardoublequoteopen}{\isasymsigma}\ {\isachargreater}\ {\isadigit{0}}{\isachardoublequoteclose}\ \isakeyword{and}\isanewline
\ \ \ \ X{\isacharunderscore}square{\isacharunderscore}integrable{\isacharcolon}\ {\isachardoublequoteopen}{\isasymAnd}n{\isachardot}\ integrable\ M\ {\isacharparenleft}{\isasymlambda}x{\isachardot}\ {\isacharparenleft}X\ n\ x{\isacharparenright}\isactrlsup {\isadigit{2}}{\isacharparenright}{\isachardoublequoteclose}\ \isakeyword{and}\isanewline
\ \ \ \ X{\isacharunderscore}variance{\isacharcolon}\ {\isachardoublequoteopen}{\isasymAnd}n{\isachardot}\ variance\ {\isacharparenleft}X\ n{\isacharparenright}\ {\isacharequal}\ {\isasymsigma}\isactrlsup {\isadigit{2}}{\isachardoublequoteclose}\ \isakeyword{and}\isanewline
\ \ \ \ X{\isacharunderscore}distrib{\isacharcolon}\ {\isachardoublequoteopen}{\isasymAnd}n{\isachardot}\ distr\ M\ borel\ {\isacharparenleft}X\ n{\isacharparenright}\ {\isacharequal}\ {\isasymmu}{\isachardoublequoteclose}\isanewline
\ \ \isakeyword{defines}\isanewline
\ \ \ \ {\isachardoublequoteopen}S\ n\ {\isasymequiv}\ {\isasymlambda}x{\isachardot}\ {\isasymSum}i{\isacharless}n{\isachardot}\ X\ i\ x{\isachardoublequoteclose}\isanewline
\ \ \isakeyword{shows}\isanewline
\ \ \ \ {\isachardoublequoteopen}weak{\isacharunderscore}conv{\isacharunderscore}m\ {\isacharparenleft}{\isasymlambda}n{\isachardot}\ distr\ M\ borel\ {\isacharparenleft}{\isasymlambda}x{\isachardot}\ S\ n\ x\ {\isacharslash}\ sqrt\ {\isacharparenleft}n\ {\isacharasterisk}\ {\isasymsigma}\isactrlsup {\isadigit{2}}{\isacharparenright}{\isacharparenright}{\isacharparenright}\ \isanewline
\ \ \ \ \ \ \ \ {\isacharparenleft}density\ lborel\ std{\isacharunderscore}normal{\isacharunderscore}density{\isacharparenright}{\isachardoublequoteclose}\isanewline
\isacommand{proof}\isamarkupfalse%
\ {\isacharminus}\isanewline
\ \ \isacommand{def}\isamarkupfalse%
\ S{\isacharprime}\ {\isasymequiv}\ {\isachardoublequoteopen}{\isasymlambda}n\ x{\isachardot}\ S\ n\ x\ {\isacharslash}\ sqrt\ {\isacharparenleft}n\ {\isacharasterisk}\ {\isasymsigma}\isactrlsup {\isadigit{2}}{\isacharparenright}{\isachardoublequoteclose}\isanewline
\ \ \isacommand{def}\isamarkupfalse%
\ {\isasymphi}\ {\isasymequiv}\ {\isachardoublequoteopen}{\isasymlambda}n{\isachardot}\ char\ {\isacharparenleft}distr\ M\ borel\ {\isacharparenleft}S{\isacharprime}\ n{\isacharparenright}{\isacharparenright}{\isachardoublequoteclose}\isanewline
\ \ \isacommand{def}\isamarkupfalse%
\ {\isasympsi}\ {\isasymequiv}\ {\isachardoublequoteopen}{\isasymlambda}n\ t{\isachardot}\ char\ {\isasymmu}\ {\isacharparenleft}t\ {\isacharslash}\ sqrt\ {\isacharparenleft}{\isasymsigma}\isactrlsup {\isadigit{2}}\ {\isacharasterisk}\ n{\isacharparenright}{\isacharparenright}{\isachardoublequoteclose}\isanewline
\isanewline
\ \ \isacommand{have}\isamarkupfalse%
\ X{\isacharunderscore}rv\ {\isacharbrackleft}simp{\isacharcomma}\ measurable{\isacharbrackright}{\isacharcolon}\ {\isachardoublequoteopen}{\isasymAnd}n{\isachardot}\ random{\isacharunderscore}variable\ borel\ {\isacharparenleft}X\ n{\isacharparenright}{\isachardoublequoteclose}\isanewline
\ \ \ \ \isacommand{using}\isamarkupfalse%
\ X{\isacharunderscore}indep\ \isacommand{unfolding}\isamarkupfalse%
\ indep{\isacharunderscore}vars{\isacharunderscore}def{\isadigit{2}}\ \isacommand{by}\isamarkupfalse%
\ simp\isanewline
\ \ \isacommand{interpret}\isamarkupfalse%
\ {\isasymmu}{\isacharcolon}\ real{\isacharunderscore}distribution\ {\isasymmu}\isanewline
\ \ \ \ \isacommand{by}\isamarkupfalse%
\ {\isacharparenleft}subst\ X{\isacharunderscore}distrib\ {\isacharbrackleft}symmetric{\isacharcomma}\ of\ {\isadigit{0}}{\isacharbrackright}{\isacharcomma}\ rule\ real{\isacharunderscore}distribution{\isacharunderscore}distr{\isacharcomma}\ simp{\isacharparenright}\isanewline
\ \ \isanewline
\ \ \isacommand{have}\isamarkupfalse%
\ {\isasymmu}{\isacharunderscore}integrable\ {\isacharbrackleft}simp{\isacharbrackright}{\isacharcolon}\ {\isachardoublequoteopen}integrable\ {\isasymmu}\ {\isacharparenleft}{\isasymlambda}x{\isachardot}\ x{\isacharparenright}{\isachardoublequoteclose}\isanewline
\ \ \ \ \isacommand{apply}\isamarkupfalse%
\ {\isacharparenleft}simp\ add{\isacharcolon}\ X{\isacharunderscore}distrib\ {\isacharbrackleft}symmetric{\isacharcomma}\ of\ {\isadigit{0}}{\isacharbrackright}{\isacharparenright}\isanewline
\ \ \ \ \isacommand{using}\isamarkupfalse%
\ assms\ \isacommand{by}\isamarkupfalse%
\ {\isacharparenleft}subst\ integrable{\isacharunderscore}distr{\isacharunderscore}eq{\isacharcomma}\ auto{\isacharparenright}\isanewline
\ \ \isacommand{have}\isamarkupfalse%
\ {\isasymmu}{\isacharunderscore}mean{\isacharunderscore}integrable\ {\isacharbrackleft}simp{\isacharbrackright}{\isacharcolon}\ {\isachardoublequoteopen}{\isasymmu}{\isachardot}expectation\ {\isacharparenleft}{\isasymlambda}x{\isachardot}\ x{\isacharparenright}\ {\isacharequal}\ {\isadigit{0}}{\isachardoublequoteclose}\isanewline
\ \ \ \ \isacommand{apply}\isamarkupfalse%
\ {\isacharparenleft}simp\ add{\isacharcolon}\ X{\isacharunderscore}distrib\ {\isacharbrackleft}symmetric{\isacharcomma}\ of\ {\isadigit{0}}{\isacharbrackright}{\isacharparenright}\isanewline
\ \ \ \ \isacommand{using}\isamarkupfalse%
\ assms\ \isacommand{by}\isamarkupfalse%
\ {\isacharparenleft}subst\ integral{\isacharunderscore}distr{\isacharcomma}\ auto{\isacharparenright}\isanewline
\ \ \isacommand{have}\isamarkupfalse%
\ {\isasymmu}{\isacharunderscore}square{\isacharunderscore}integrable\ {\isacharbrackleft}simp{\isacharbrackright}{\isacharcolon}\ {\isachardoublequoteopen}integrable\ {\isasymmu}\ {\isacharparenleft}{\isasymlambda}x{\isachardot}\ x{\isacharcircum}{\isadigit{2}}{\isacharparenright}{\isachardoublequoteclose}\isanewline
\ \ \ \ \isacommand{apply}\isamarkupfalse%
\ {\isacharparenleft}simp\ add{\isacharcolon}\ X{\isacharunderscore}distrib\ {\isacharbrackleft}symmetric{\isacharcomma}\ of\ {\isadigit{0}}{\isacharbrackright}{\isacharparenright}\isanewline
\ \ \ \ \isacommand{using}\isamarkupfalse%
\ assms\ \isacommand{by}\isamarkupfalse%
\ {\isacharparenleft}subst\ integrable{\isacharunderscore}distr{\isacharunderscore}eq{\isacharcomma}\ auto{\isacharparenright}\isanewline
\ \ \isacommand{have}\isamarkupfalse%
\ {\isasymmu}{\isacharunderscore}variance\ {\isacharbrackleft}simp{\isacharbrackright}{\isacharcolon}\ {\isachardoublequoteopen}{\isasymmu}{\isachardot}expectation\ {\isacharparenleft}{\isasymlambda}x{\isachardot}\ x{\isacharcircum}{\isadigit{2}}{\isacharparenright}\ {\isacharequal}\ {\isasymsigma}\isactrlsup {\isadigit{2}}{\isachardoublequoteclose}\isanewline
\ \ \ \ \isacommand{apply}\isamarkupfalse%
\ {\isacharparenleft}simp\ add{\isacharcolon}\ X{\isacharunderscore}distrib\ {\isacharbrackleft}symmetric{\isacharcomma}\ of\ {\isadigit{0}}{\isacharbrackright}{\isacharparenright}\isanewline
\ \ \ \ \isacommand{using}\isamarkupfalse%
\ assms\ \isacommand{by}\isamarkupfalse%
\ {\isacharparenleft}subst\ integral{\isacharunderscore}distr{\isacharcomma}\ auto{\isacharparenright}\isanewline
\isanewline
\ \ \isacommand{have}\isamarkupfalse%
\ main{\isacharcolon}\ {\isachardoublequoteopen}{\isasymAnd}t{\isachardot}\ eventually\ {\isacharparenleft}{\isasymlambda}n{\isachardot}\ cmod\ {\isacharparenleft}{\isasymphi}\ n\ t\ {\isacharminus}\ {\isacharparenleft}{\isadigit{1}}\ {\isacharplus}\ {\isacharparenleft}{\isacharminus}{\isacharparenleft}t{\isacharcircum}{\isadigit{2}}{\isacharparenright}\ {\isacharslash}\ {\isadigit{2}}{\isacharparenright}\ {\isacharslash}\ n{\isacharparenright}{\isacharcircum}n{\isacharparenright}\ {\isasymle}\ \isanewline
\ \ \ \ \ \ \ \ {\isacharparenleft}t\isactrlsup {\isadigit{2}}\ {\isacharslash}\ {\isacharparenleft}{\isadigit{6}}\ {\isacharasterisk}\ {\isasymsigma}\isactrlsup {\isadigit{2}}{\isacharparenright}\ {\isacharasterisk}\ {\isacharparenleft}LINT\ x{\isacharbar}{\isasymmu}{\isachardot}\ min\ {\isacharparenleft}{\isadigit{6}}\ {\isacharasterisk}\ x\isactrlsup {\isadigit{2}}{\isacharparenright}\ {\isacharparenleft}{\isasymbar}t\ {\isacharslash}\ sqrt\ {\isacharparenleft}{\isasymsigma}\isactrlsup {\isadigit{2}}\ {\isacharasterisk}\ n{\isacharparenright}{\isasymbar}\ {\isacharasterisk}\ {\isasymbar}x{\isasymbar}\ {\isacharcircum}\ {\isadigit{3}}{\isacharparenright}{\isacharparenright}{\isacharparenright}{\isacharparenright}\isanewline
\ \ \ \ \ \ sequentially{\isachardoublequoteclose}\isanewline
\ \ \isacommand{proof}\isamarkupfalse%
\ {\isacharparenleft}rule\ eventually{\isacharunderscore}sequentiallyI{\isacharparenright}\isanewline
\ \ \ \ \isacommand{fix}\isamarkupfalse%
\ n\ {\isacharcolon}{\isacharcolon}\ nat\ \isakeyword{and}\ t\ {\isacharcolon}{\isacharcolon}\ real\isanewline
\ \ \ \ \isacommand{assume}\isamarkupfalse%
\ {\isachardoublequoteopen}n\ {\isasymge}\ nat\ {\isacharparenleft}ceiling\ {\isacharparenleft}t{\isacharcircum}{\isadigit{2}}\ {\isacharslash}\ {\isadigit{4}}{\isacharparenright}{\isacharparenright}{\isachardoublequoteclose}\isanewline
\ \ \ \ \isacommand{hence}\isamarkupfalse%
\ n{\isacharcolon}\ {\isachardoublequoteopen}n\ {\isasymge}\ t{\isacharcircum}{\isadigit{2}}\ {\isacharslash}\ {\isadigit{4}}{\isachardoublequoteclose}\ \isacommand{by}\isamarkupfalse%
\ {\isacharparenleft}subst\ nat{\isacharunderscore}ceiling{\isacharunderscore}le{\isacharunderscore}eq\ {\isacharbrackleft}symmetric{\isacharbrackright}{\isacharparenright}\isanewline
\ \ \ \ \isacommand{let}\isamarkupfalse%
\ {\isacharquery}t\ {\isacharequal}\ {\isachardoublequoteopen}t\ {\isacharslash}\ sqrt\ {\isacharparenleft}{\isasymsigma}\isactrlsup {\isadigit{2}}\ {\isacharasterisk}\ n{\isacharparenright}{\isachardoublequoteclose}\isanewline
\isanewline
\ \ \ \ \isacommand{def}\isamarkupfalse%
\ {\isasympsi}{\isacharprime}\ {\isasymequiv}\ {\isachardoublequoteopen}{\isasymlambda}n\ i{\isachardot}\ char\ {\isacharparenleft}distr\ M\ borel\ {\isacharparenleft}{\isasymlambda}x{\isachardot}\ X\ i\ x\ {\isacharslash}\ sqrt\ {\isacharparenleft}{\isasymsigma}\isactrlsup {\isadigit{2}}\ {\isacharasterisk}\ n{\isacharparenright}{\isacharparenright}{\isacharparenright}{\isachardoublequoteclose}\isanewline
\ \ \ \ \isacommand{have}\isamarkupfalse%
\ {\isacharasterisk}{\isacharcolon}\ {\isachardoublequoteopen}{\isasymAnd}n\ i\ t{\isachardot}\ {\isasympsi}{\isacharprime}\ n\ i\ t\ {\isacharequal}\ {\isasympsi}\ n\ t{\isachardoublequoteclose}\isanewline
\ \ \ \ \ \ \isacommand{unfolding}\isamarkupfalse%
\ {\isasympsi}{\isacharunderscore}def\ {\isasympsi}{\isacharprime}{\isacharunderscore}def\ char{\isacharunderscore}def\ \isacommand{apply}\isamarkupfalse%
\ auto\isanewline
\ \ \ \ \ \ \isacommand{apply}\isamarkupfalse%
\ {\isacharparenleft}subst\ X{\isacharunderscore}distrib\ {\isacharbrackleft}symmetric{\isacharbrackright}{\isacharparenright}\isanewline
\ \ \ \ \ \ \isacommand{apply}\isamarkupfalse%
\ {\isacharparenleft}subst\ integral{\isacharunderscore}distr{\isacharcomma}\ auto{\isacharparenright}\isanewline
\ \ \ \ \ \ \isacommand{by}\isamarkupfalse%
\ {\isacharparenleft}subst\ integral{\isacharunderscore}distr{\isacharcomma}\ auto{\isacharparenright}\isanewline
\isanewline
\ \ \ \ \isacommand{have}\isamarkupfalse%
\ {\isadigit{1}}{\isacharcolon}\ {\isachardoublequoteopen}S{\isacharprime}\ n\ {\isacharequal}\ {\isacharparenleft}{\isasymlambda}x{\isachardot}\ {\isacharparenleft}{\isasymSum}\ i\ {\isacharless}\ n{\isachardot}\ X\ i\ x\ {\isacharslash}\ sqrt\ {\isacharparenleft}{\isasymsigma}\isactrlsup {\isadigit{2}}\ {\isacharasterisk}\ n{\isacharparenright}{\isacharparenright}{\isacharparenright}{\isachardoublequoteclose}\ \isanewline
\ \ \ \ \ \ \isacommand{by}\isamarkupfalse%
\ {\isacharparenleft}rule\ ext{\isacharcomma}\ simp\ add{\isacharcolon}\ S{\isacharprime}{\isacharunderscore}def\ S{\isacharunderscore}def\ setsum{\isacharunderscore}divide{\isacharunderscore}distrib\ ac{\isacharunderscore}simps{\isacharparenright}\isanewline
\isanewline
\ \ \ \ \isacommand{have}\isamarkupfalse%
\ {\isachardoublequoteopen}{\isasymphi}\ n\ t\ {\isacharequal}\ {\isacharparenleft}{\isasymProd}\ i\ {\isacharless}\ n{\isachardot}\ {\isasympsi}{\isacharprime}\ n\ i\ t{\isacharparenright}{\isachardoublequoteclose}\isanewline
\ \ \ \ \ \ \isacommand{unfolding}\isamarkupfalse%
\ {\isasymphi}{\isacharunderscore}def\ {\isasympsi}{\isacharprime}{\isacharunderscore}def\ \isacommand{apply}\isamarkupfalse%
\ {\isacharparenleft}subst\ {\isadigit{1}}{\isacharparenright}\isanewline
\ \ \ \ \ \ \isacommand{apply}\isamarkupfalse%
\ {\isacharparenleft}rule\ char{\isacharunderscore}distr{\isacharunderscore}setsum{\isacharparenright}\isanewline
\ \ \ \ \ \ \isacommand{apply}\isamarkupfalse%
\ {\isacharparenleft}rule\ indep{\isacharunderscore}vars{\isacharunderscore}compose{\isadigit{2}}{\isacharbrackleft}\isakeyword{where}\ X{\isacharequal}X{\isacharbrackright}{\isacharparenright}\isanewline
\ \ \ \ \ \ \isacommand{apply}\isamarkupfalse%
\ {\isacharparenleft}rule\ indep{\isacharunderscore}vars{\isacharunderscore}subset{\isacharparenright}\isanewline
\ \ \ \ \ \ \isacommand{apply}\isamarkupfalse%
\ {\isacharparenleft}rule\ X{\isacharunderscore}indep{\isacharparenright}\isanewline
\ \ \ \ \ \ \isacommand{apply}\isamarkupfalse%
\ auto\isanewline
\ \ \ \ \ \ \isacommand{done}\isamarkupfalse%
\isanewline
\ \ \ \ \isacommand{also}\isamarkupfalse%
\ \isacommand{have}\isamarkupfalse%
\ {\isachardoublequoteopen}{\isasymdots}\ {\isacharequal}\ {\isacharparenleft}{\isasympsi}\ n\ t{\isacharparenright}{\isacharcircum}n{\isachardoublequoteclose}\isanewline
\ \ \ \ \ \ \isacommand{by}\isamarkupfalse%
\ {\isacharparenleft}auto\ simp\ add{\isacharcolon}\ {\isacharasterisk}\ setprod{\isacharunderscore}constant{\isacharparenright}\isanewline
\ \ \ \ \isacommand{finally}\isamarkupfalse%
\ \isacommand{have}\isamarkupfalse%
\ {\isadigit{2}}{\isacharcolon}\ {\isachardoublequoteopen}{\isasymphi}\ n\ t\ {\isacharequal}{\isacharparenleft}{\isasympsi}\ n\ t{\isacharparenright}{\isacharcircum}n{\isachardoublequoteclose}\ \isacommand{{\isachardot}}\isamarkupfalse%
\isanewline
\isanewline
\ \ \ \ \isacommand{have}\isamarkupfalse%
\ {\isachardoublequoteopen}cmod\ {\isacharparenleft}{\isasympsi}\ n\ t\ {\isacharminus}\ {\isacharparenleft}{\isadigit{1}}\ {\isacharminus}\ {\isacharquery}t{\isacharcircum}{\isadigit{2}}\ {\isacharasterisk}\ {\isasymsigma}\isactrlsup {\isadigit{2}}\ {\isacharslash}\ {\isadigit{2}}{\isacharparenright}{\isacharparenright}\ {\isasymle}\ \isanewline
\ \ \ \ \ \ \ \ {\isacharquery}t\isactrlsup {\isadigit{2}}\ {\isacharslash}\ {\isadigit{6}}\ {\isacharasterisk}\ {\isacharparenleft}LINT\ x{\isacharbar}{\isasymmu}{\isachardot}\ min\ {\isacharparenleft}{\isadigit{6}}\ {\isacharasterisk}\ x\isactrlsup {\isadigit{2}}{\isacharparenright}\ {\isacharparenleft}{\isasymbar}{\isacharquery}t{\isasymbar}\ {\isacharasterisk}\ {\isasymbar}x{\isasymbar}\ {\isacharcircum}\ {\isadigit{3}}{\isacharparenright}{\isacharparenright}{\isachardoublequoteclose}\isanewline
\ \ \ \ \ \ \isacommand{unfolding}\isamarkupfalse%
\ {\isasympsi}{\isacharunderscore}def\ \isacommand{by}\isamarkupfalse%
\ {\isacharparenleft}rule\ {\isasymmu}{\isachardot}aux{\isacharcomma}\ auto{\isacharparenright}\isanewline
\ \ \ \ \isacommand{also}\isamarkupfalse%
\ \isacommand{have}\isamarkupfalse%
\ {\isachardoublequoteopen}{\isacharquery}t{\isacharcircum}{\isadigit{2}}\ {\isacharasterisk}\ {\isasymsigma}\isactrlsup {\isadigit{2}}\ {\isacharequal}\ t{\isacharcircum}{\isadigit{2}}\ {\isacharslash}\ n{\isachardoublequoteclose}\isanewline
\ \ \ \ \ \ \isacommand{using}\isamarkupfalse%
\ {\isasymsigma}{\isacharunderscore}pos\ \isacommand{by}\isamarkupfalse%
\ {\isacharparenleft}simp\ add{\isacharcolon}\ power{\isacharunderscore}divide{\isacharparenright}\isanewline
\ \ \ \ \isacommand{also}\isamarkupfalse%
\ \isacommand{have}\isamarkupfalse%
\ {\isachardoublequoteopen}t{\isacharcircum}{\isadigit{2}}\ {\isacharslash}\ n\ {\isacharslash}\ {\isadigit{2}}\ {\isacharequal}\ {\isacharparenleft}t{\isacharcircum}{\isadigit{2}}\ {\isacharslash}\ {\isadigit{2}}{\isacharparenright}\ {\isacharslash}\ n{\isachardoublequoteclose}\ \isacommand{by}\isamarkupfalse%
\ simp\isanewline
\ \ \ \ \isacommand{finally}\isamarkupfalse%
\ \isacommand{have}\isamarkupfalse%
\ {\isacharasterisk}{\isacharasterisk}{\isacharcolon}\ {\isachardoublequoteopen}cmod\ {\isacharparenleft}{\isasympsi}\ n\ t\ {\isacharminus}\ {\isacharparenleft}{\isadigit{1}}\ {\isacharplus}\ {\isacharparenleft}{\isacharminus}{\isacharparenleft}t{\isacharcircum}{\isadigit{2}}{\isacharparenright}\ {\isacharslash}\ {\isadigit{2}}{\isacharparenright}\ {\isacharslash}\ n{\isacharparenright}{\isacharparenright}\ {\isasymle}\ \isanewline
\ \ \ \ \ \ {\isacharquery}t\isactrlsup {\isadigit{2}}\ {\isacharslash}\ {\isadigit{6}}\ {\isacharasterisk}\ {\isacharparenleft}LINT\ x{\isacharbar}{\isasymmu}{\isachardot}\ min\ {\isacharparenleft}{\isadigit{6}}\ {\isacharasterisk}\ x\isactrlsup {\isadigit{2}}{\isacharparenright}\ {\isacharparenleft}{\isasymbar}{\isacharquery}t{\isasymbar}\ {\isacharasterisk}\ {\isasymbar}x{\isasymbar}\ {\isacharcircum}\ {\isadigit{3}}{\isacharparenright}{\isacharparenright}{\isachardoublequoteclose}\ \isacommand{by}\isamarkupfalse%
\ simp\isanewline
\isanewline
\ \ \ \ \isacommand{have}\isamarkupfalse%
\ {\isachardoublequoteopen}cmod\ {\isacharparenleft}{\isasymphi}\ n\ t\ {\isacharminus}\ {\isacharparenleft}complex{\isacharunderscore}of{\isacharunderscore}real\ {\isacharparenleft}{\isadigit{1}}\ {\isacharplus}\ {\isacharparenleft}{\isacharminus}{\isacharparenleft}t{\isacharcircum}{\isadigit{2}}{\isacharparenright}\ {\isacharslash}\ {\isadigit{2}}{\isacharparenright}\ {\isacharslash}\ n{\isacharparenright}{\isacharparenright}{\isacharcircum}n{\isacharparenright}\ {\isasymle}\ \isanewline
\ \ \ \ \ \ \ \ \ n\ {\isacharasterisk}\ cmod\ {\isacharparenleft}{\isasympsi}\ n\ t\ {\isacharminus}\ {\isacharparenleft}complex{\isacharunderscore}of{\isacharunderscore}real\ {\isacharparenleft}{\isadigit{1}}\ {\isacharplus}\ {\isacharparenleft}{\isacharminus}{\isacharparenleft}t{\isacharcircum}{\isadigit{2}}{\isacharparenright}\ {\isacharslash}\ {\isadigit{2}}{\isacharparenright}\ {\isacharslash}\ n{\isacharparenright}{\isacharparenright}{\isacharparenright}{\isachardoublequoteclose}\isanewline
\ \ \ \ \ \ \isacommand{apply}\isamarkupfalse%
\ {\isacharparenleft}subst\ {\isadigit{2}}{\isacharcomma}\ rule\ norm{\isacharunderscore}power{\isacharunderscore}diff{\isacharparenright}\isanewline
\ \ \ \ \ \ \isacommand{unfolding}\isamarkupfalse%
\ {\isasympsi}{\isacharunderscore}def\ \isacommand{apply}\isamarkupfalse%
\ {\isacharparenleft}rule\ {\isasymmu}{\isachardot}cmod{\isacharunderscore}char{\isacharunderscore}le{\isacharunderscore}{\isadigit{1}}{\isacharparenright}\isanewline
\ \ \ \ \ \ \isacommand{apply}\isamarkupfalse%
\ {\isacharparenleft}simp\ only{\isacharcolon}\ norm{\isacharunderscore}of{\isacharunderscore}real{\isacharparenright}\isanewline
\ \ \ \ \ \ \isacommand{apply}\isamarkupfalse%
\ {\isacharparenleft}auto\ intro{\isacharbang}{\isacharcolon}\ abs{\isacharunderscore}leI{\isacharparenright}\isanewline
\ \ \ \ \ \ \isacommand{using}\isamarkupfalse%
\ n\ \isacommand{by}\isamarkupfalse%
\ {\isacharparenleft}subst\ divide{\isacharunderscore}le{\isacharunderscore}eq{\isacharcomma}\ auto{\isacharparenright}\isanewline
\ \ \ \ \isacommand{also}\isamarkupfalse%
\ \isacommand{have}\isamarkupfalse%
\ {\isachardoublequoteopen}{\isasymdots}\ {\isasymle}\ n\ {\isacharasterisk}\ {\isacharparenleft}{\isacharquery}t\isactrlsup {\isadigit{2}}\ {\isacharslash}\ {\isadigit{6}}\ {\isacharasterisk}\ {\isacharparenleft}LINT\ x{\isacharbar}{\isasymmu}{\isachardot}\ min\ {\isacharparenleft}{\isadigit{6}}\ {\isacharasterisk}\ x\isactrlsup {\isadigit{2}}{\isacharparenright}\ {\isacharparenleft}{\isasymbar}{\isacharquery}t{\isasymbar}\ {\isacharasterisk}\ {\isasymbar}x{\isasymbar}\ {\isacharcircum}\ {\isadigit{3}}{\isacharparenright}{\isacharparenright}{\isacharparenright}{\isachardoublequoteclose}\isanewline
\ \ \ \ \ \ \isacommand{by}\isamarkupfalse%
\ {\isacharparenleft}rule\ mult{\isacharunderscore}left{\isacharunderscore}mono\ {\isacharbrackleft}OF\ {\isacharasterisk}{\isacharasterisk}{\isacharbrackright}{\isacharcomma}\ simp{\isacharparenright}\isanewline
\ \ \ \ \isacommand{also}\isamarkupfalse%
\ \isacommand{have}\isamarkupfalse%
\ {\isachardoublequoteopen}{\isasymdots}\ {\isacharequal}\ {\isacharparenleft}t\isactrlsup {\isadigit{2}}\ {\isacharslash}\ {\isacharparenleft}{\isadigit{6}}\ {\isacharasterisk}\ {\isasymsigma}\isactrlsup {\isadigit{2}}{\isacharparenright}\ {\isacharasterisk}\ {\isacharparenleft}LINT\ x{\isacharbar}{\isasymmu}{\isachardot}\ min\ {\isacharparenleft}{\isadigit{6}}\ {\isacharasterisk}\ x\isactrlsup {\isadigit{2}}{\isacharparenright}\ {\isacharparenleft}{\isasymbar}{\isacharquery}t{\isasymbar}\ {\isacharasterisk}\ {\isasymbar}x{\isasymbar}\ {\isacharcircum}\ {\isadigit{3}}{\isacharparenright}{\isacharparenright}{\isacharparenright}{\isachardoublequoteclose}\ \isanewline
\ \ \ \ \ \ \isacommand{using}\isamarkupfalse%
\ {\isasymsigma}{\isacharunderscore}pos\ \isacommand{by}\isamarkupfalse%
\ {\isacharparenleft}simp\ add{\isacharcolon}\ field{\isacharunderscore}simps\ min{\isacharunderscore}absorb{\isadigit{2}}{\isacharparenright}\isanewline
\ \ \ \ \isacommand{finally}\isamarkupfalse%
\ \isacommand{show}\isamarkupfalse%
\ {\isachardoublequoteopen}cmod\ {\isacharparenleft}{\isasymphi}\ n\ t\ {\isacharminus}\ {\isacharparenleft}{\isadigit{1}}\ {\isacharplus}\ {\isacharparenleft}{\isacharminus}{\isacharparenleft}t{\isacharcircum}{\isadigit{2}}{\isacharparenright}\ {\isacharslash}\ {\isadigit{2}}{\isacharparenright}\ {\isacharslash}\ n{\isacharparenright}{\isacharcircum}n{\isacharparenright}\ {\isasymle}\ \isanewline
\ \ \ \ \ \ \ \ {\isacharparenleft}t\isactrlsup {\isadigit{2}}\ {\isacharslash}\ {\isacharparenleft}{\isadigit{6}}\ {\isacharasterisk}\ {\isasymsigma}\isactrlsup {\isadigit{2}}{\isacharparenright}\ {\isacharasterisk}\ {\isacharparenleft}LINT\ x{\isacharbar}{\isasymmu}{\isachardot}\ min\ {\isacharparenleft}{\isadigit{6}}\ {\isacharasterisk}\ x\isactrlsup {\isadigit{2}}{\isacharparenright}\ {\isacharparenleft}{\isasymbar}{\isacharquery}t{\isasymbar}\ {\isacharasterisk}\ {\isasymbar}x{\isasymbar}\ {\isacharcircum}\ {\isadigit{3}}{\isacharparenright}{\isacharparenright}{\isacharparenright}{\isachardoublequoteclose}\ \isanewline
\ \ \ \ \ \ \isacommand{by}\isamarkupfalse%
\ simp\isanewline
\ \ \isacommand{qed}\isamarkupfalse%
\isanewline
\isanewline
\ \ \isacommand{have}\isamarkupfalse%
\ S{\isacharunderscore}rv\ {\isacharbrackleft}simp{\isacharcomma}\ measurable{\isacharbrackright}{\isacharcolon}\ {\isachardoublequoteopen}{\isasymAnd}n{\isachardot}\ random{\isacharunderscore}variable\ borel\ {\isacharparenleft}{\isasymlambda}x{\isachardot}\ S\ n\ x\ {\isacharslash}\ sqrt\ {\isacharparenleft}n\ {\isacharasterisk}\ {\isasymsigma}\isactrlsup {\isadigit{2}}{\isacharparenright}{\isacharparenright}{\isachardoublequoteclose}\isanewline
\ \ \ \ \isacommand{unfolding}\isamarkupfalse%
\ S{\isacharunderscore}def\ \isacommand{by}\isamarkupfalse%
\ measurable\isanewline
\ \ \isacommand{have}\isamarkupfalse%
\ {\isachardoublequoteopen}{\isasymAnd}t{\isachardot}\ {\isacharparenleft}{\isasymlambda}n{\isachardot}\ {\isasymphi}\ n\ t{\isacharparenright}\ {\isacharminus}{\isacharminus}{\isacharminus}{\isacharminus}{\isachargreater}\ char\ std{\isacharunderscore}normal{\isacharunderscore}distribution\ t{\isachardoublequoteclose}\isanewline
\ \ \isacommand{proof}\isamarkupfalse%
\ {\isacharminus}\isanewline
\ \ \ \ \isacommand{fix}\isamarkupfalse%
\ t\isanewline
\ \ \ \ \isacommand{let}\isamarkupfalse%
\ {\isacharquery}t\ {\isacharequal}\ {\isachardoublequoteopen}{\isasymlambda}n{\isachardot}\ t\ {\isacharslash}\ sqrt\ {\isacharparenleft}{\isasymsigma}\isactrlsup {\isadigit{2}}\ {\isacharasterisk}\ n{\isacharparenright}{\isachardoublequoteclose}\isanewline
\ \ \ \ \isacommand{have}\isamarkupfalse%
\ {\isacharasterisk}{\isacharcolon}\ {\isachardoublequoteopen}{\isasymAnd}n{\isachardot}\ integrable\ {\isasymmu}\ {\isacharparenleft}{\isasymlambda}x{\isachardot}\ {\isadigit{6}}\ {\isacharasterisk}\ x{\isacharcircum}{\isadigit{2}}{\isacharparenright}{\isachardoublequoteclose}\ \isacommand{by}\isamarkupfalse%
\ auto\isanewline
\ \ \ \ \isacommand{have}\isamarkupfalse%
\ {\isacharasterisk}{\isacharasterisk}{\isacharcolon}\ {\isachardoublequoteopen}{\isasymAnd}n{\isachardot}\ integrable\ {\isasymmu}\ {\isacharparenleft}{\isasymlambda}x{\isachardot}\ min\ {\isacharparenleft}{\isadigit{6}}\ {\isacharasterisk}\ x\isactrlsup {\isadigit{2}}{\isacharparenright}\ {\isacharparenleft}{\isasymbar}t\ {\isacharslash}\ sqrt\ {\isacharparenleft}{\isasymsigma}\isactrlsup {\isadigit{2}}\ {\isacharasterisk}\ real\ n{\isacharparenright}{\isasymbar}\ {\isacharasterisk}\ {\isasymbar}x{\isasymbar}\ {\isacharcircum}\ {\isadigit{3}}{\isacharparenright}{\isacharparenright}{\isachardoublequoteclose}\isanewline
\ \ \ \ \ \ \isacommand{by}\isamarkupfalse%
\ {\isacharparenleft}rule\ integrable{\isacharunderscore}bound\ {\isacharbrackleft}OF\ {\isacharasterisk}{\isacharbrackright}{\isacharparenright}\ auto\isanewline
\ \ \ \ \isacommand{have}\isamarkupfalse%
\ {\isacharasterisk}{\isacharasterisk}{\isacharasterisk}{\isacharcolon}\ {\isachardoublequoteopen}{\isasymAnd}x{\isachardot}\ {\isacharparenleft}{\isasymlambda}n{\isachardot}\ {\isasymbar}t{\isasymbar}\ {\isacharasterisk}\ {\isasymbar}x{\isasymbar}\ {\isacharcircum}\ {\isadigit{3}}\ {\isacharslash}\ {\isasymbar}sqrt\ {\isacharparenleft}{\isasymsigma}\isactrlsup {\isadigit{2}}\ {\isacharasterisk}\ real\ n{\isacharparenright}{\isasymbar}{\isacharparenright}\ {\isacharminus}{\isacharminus}{\isacharminus}{\isacharminus}{\isachargreater}\ {\isadigit{0}}{\isachardoublequoteclose}\isanewline
\ \ \ \ \ \ \isacommand{apply}\isamarkupfalse%
\ {\isacharparenleft}subst\ divide{\isacharunderscore}inverse{\isacharparenright}\isanewline
\ \ \ \ \ \ \isacommand{apply}\isamarkupfalse%
\ {\isacharparenleft}rule\ tendsto{\isacharunderscore}mult{\isacharunderscore}right{\isacharunderscore}zero{\isacharparenright}\isanewline
\ \ \ \ \ \ \isacommand{using}\isamarkupfalse%
\ {\isasymsigma}{\isacharunderscore}pos\ \isacommand{apply}\isamarkupfalse%
\ {\isacharparenleft}subst\ abs{\isacharunderscore}of{\isacharunderscore}nonneg{\isacharcomma}\ simp{\isacharparenright}\isanewline
\ \ \ \ \ \ \isacommand{apply}\isamarkupfalse%
\ {\isacharparenleft}simp\ add{\isacharcolon}\ real{\isacharunderscore}sqrt{\isacharunderscore}mult{\isacharparenright}\isanewline
\ \ \ \ \ \ \isacommand{apply}\isamarkupfalse%
\ {\isacharparenleft}rule\ tendsto{\isacharunderscore}mult{\isacharunderscore}right{\isacharunderscore}zero{\isacharparenright}\isanewline
\ \ \ \ \ \ \isacommand{apply}\isamarkupfalse%
\ {\isacharparenleft}rule\ tendsto{\isacharunderscore}inverse{\isacharunderscore}{\isadigit{0}}{\isacharunderscore}at{\isacharunderscore}top{\isacharparenright}\isanewline
\ \ \ \ \ \ \isacommand{by}\isamarkupfalse%
\ {\isacharparenleft}rule\ filterlim{\isacharunderscore}compose\ {\isacharbrackleft}OF\ sqrt{\isacharunderscore}at{\isacharunderscore}top\ filterlim{\isacharunderscore}real{\isacharunderscore}sequentially{\isacharbrackright}{\isacharparenright}\isanewline
\ \ \ \ \isacommand{have}\isamarkupfalse%
\ {\isachardoublequoteopen}{\isacharparenleft}{\isasymlambda}n{\isachardot}\ LINT\ x{\isacharbar}{\isasymmu}{\isachardot}\ min\ {\isacharparenleft}{\isadigit{6}}\ {\isacharasterisk}\ x\isactrlsup {\isadigit{2}}{\isacharparenright}\ {\isacharparenleft}{\isasymbar}{\isacharquery}t\ n{\isasymbar}\ {\isacharasterisk}\ {\isasymbar}x{\isasymbar}\ {\isacharcircum}\ {\isadigit{3}}{\isacharparenright}{\isacharparenright}\ {\isacharminus}{\isacharminus}{\isacharminus}{\isacharminus}{\isachargreater}\ {\isacharparenleft}LINT\ x{\isacharbar}{\isasymmu}{\isachardot}\ {\isadigit{0}}{\isacharparenright}{\isachardoublequoteclose}\isanewline
\ \ \ \ \ \ \isacommand{apply}\isamarkupfalse%
\ {\isacharparenleft}rule\ integral{\isacharunderscore}dominated{\isacharunderscore}convergence\ {\isacharbrackleft}\isakeyword{where}\ w\ {\isacharequal}\ {\isachardoublequoteopen}{\isasymlambda}x{\isachardot}\ {\isadigit{6}}\ {\isacharasterisk}\ x{\isacharcircum}{\isadigit{2}}{\isachardoublequoteclose}{\isacharbrackright}{\isacharparenright}\isanewline
\ \ \ \ \ \ \isacommand{using}\isamarkupfalse%
\ {\isasymsigma}{\isacharunderscore}pos\ \isacommand{apply}\isamarkupfalse%
\ {\isacharparenleft}auto\ intro{\isacharbang}{\isacharcolon}\ AE{\isacharunderscore}I{\isadigit{2}}{\isacharparenright}\isanewline
\ \ \ \ \ \ \isacommand{apply}\isamarkupfalse%
\ {\isacharparenleft}rule\ tendsto{\isacharunderscore}sandwich\ {\isacharbrackleft}OF\ {\isacharunderscore}\ {\isacharunderscore}\ tendsto{\isacharunderscore}const\ {\isacharasterisk}{\isacharasterisk}{\isacharasterisk}{\isacharbrackright}{\isacharparenright}\isanewline
\ \ \ \ \ \ \isacommand{apply}\isamarkupfalse%
\ {\isacharparenleft}auto\ intro{\isacharbang}{\isacharcolon}\ always{\isacharunderscore}eventually\ min{\isachardot}cobounded{\isadigit{2}}{\isacharparenright}\isanewline
\ \ \ \ \ \ \isacommand{done}\isamarkupfalse%
\isanewline
\ \ \ \ \isacommand{hence}\isamarkupfalse%
\ {\isachardoublequoteopen}{\isacharparenleft}{\isasymlambda}n{\isachardot}\ LINT\ x{\isacharbar}{\isasymmu}{\isachardot}\ min\ {\isacharparenleft}{\isadigit{6}}\ {\isacharasterisk}\ x\isactrlsup {\isadigit{2}}{\isacharparenright}\ {\isacharparenleft}{\isasymbar}{\isacharquery}t\ n{\isasymbar}\ {\isacharasterisk}\ {\isasymbar}x{\isasymbar}\ {\isacharcircum}\ {\isadigit{3}}{\isacharparenright}{\isacharparenright}\ {\isacharminus}{\isacharminus}{\isacharminus}{\isacharminus}{\isachargreater}\ {\isadigit{0}}{\isachardoublequoteclose}\ \isacommand{by}\isamarkupfalse%
\ simp\isanewline
\ \ \ \ \isacommand{hence}\isamarkupfalse%
\ main{\isadigit{2}}{\isacharcolon}\ {\isachardoublequoteopen}{\isacharparenleft}{\isasymlambda}n{\isachardot}\ t\isactrlsup {\isadigit{2}}\ {\isacharslash}\ {\isacharparenleft}{\isadigit{6}}\ {\isacharasterisk}\ {\isasymsigma}\isactrlsup {\isadigit{2}}{\isacharparenright}\ {\isacharasterisk}\ {\isacharparenleft}LINT\ x{\isacharbar}{\isasymmu}{\isachardot}\ min\ {\isacharparenleft}{\isadigit{6}}\ {\isacharasterisk}\ x\isactrlsup {\isadigit{2}}{\isacharparenright}\ {\isacharparenleft}{\isasymbar}{\isacharquery}t\ n{\isasymbar}\ {\isacharasterisk}\ {\isasymbar}x{\isasymbar}\ {\isacharcircum}\ {\isadigit{3}}{\isacharparenright}{\isacharparenright}{\isacharparenright}\ {\isacharminus}{\isacharminus}{\isacharminus}{\isacharminus}{\isachargreater}\ {\isadigit{0}}{\isachardoublequoteclose}\isanewline
\ \ \ \ \ \ \isacommand{by}\isamarkupfalse%
\ {\isacharparenleft}rule\ tendsto{\isacharunderscore}mult{\isacharunderscore}right{\isacharunderscore}zero{\isacharparenright}\isanewline
\ \ \ \ \isacommand{have}\isamarkupfalse%
\ {\isacharasterisk}{\isacharasterisk}{\isacharcolon}\ {\isachardoublequoteopen}{\isacharparenleft}{\isasymlambda}n{\isachardot}\ {\isacharparenleft}{\isadigit{1}}\ {\isacharplus}\ {\isacharparenleft}{\isacharminus}{\isacharparenleft}t{\isacharcircum}{\isadigit{2}}{\isacharparenright}\ {\isacharslash}\ {\isadigit{2}}{\isacharparenright}\ {\isacharslash}\ n{\isacharparenright}{\isacharcircum}n{\isacharparenright}\ {\isacharminus}{\isacharminus}{\isacharminus}{\isacharminus}{\isachargreater}\ exp\ {\isacharparenleft}{\isacharminus}{\isacharparenleft}t{\isacharcircum}{\isadigit{2}}{\isacharparenright}\ {\isacharslash}\ {\isadigit{2}}{\isacharparenright}{\isachardoublequoteclose}\isanewline
\ \ \ \ \ \ \isacommand{by}\isamarkupfalse%
\ {\isacharparenleft}rule\ tendsto{\isacharunderscore}exp{\isacharunderscore}limit{\isacharunderscore}sequentially{\isacharparenright}\isanewline
\ \ \ \ \isacommand{have}\isamarkupfalse%
\ {\isachardoublequoteopen}{\isacharparenleft}{\isasymlambda}n{\isachardot}\ complex{\isacharunderscore}of{\isacharunderscore}real\ {\isacharparenleft}{\isacharparenleft}{\isadigit{1}}\ {\isacharplus}\ {\isacharparenleft}{\isacharminus}{\isacharparenleft}t{\isacharcircum}{\isadigit{2}}{\isacharparenright}\ {\isacharslash}\ {\isadigit{2}}{\isacharparenright}\ {\isacharslash}\ n{\isacharparenright}{\isacharcircum}n{\isacharparenright}{\isacharparenright}\ {\isacharminus}{\isacharminus}{\isacharminus}{\isacharminus}{\isachargreater}\ \isanewline
\ \ \ \ \ \ \ \ complex{\isacharunderscore}of{\isacharunderscore}real\ {\isacharparenleft}exp\ {\isacharparenleft}{\isacharminus}{\isacharparenleft}t{\isacharcircum}{\isadigit{2}}{\isacharparenright}\ {\isacharslash}\ {\isadigit{2}}{\isacharparenright}{\isacharparenright}{\isachardoublequoteclose}\isanewline
\ \ \ \ \ \ \isacommand{by}\isamarkupfalse%
\ {\isacharparenleft}rule\ isCont{\isacharunderscore}tendsto{\isacharunderscore}compose\ {\isacharbrackleft}OF\ {\isacharunderscore}\ {\isacharasterisk}{\isacharasterisk}{\isacharbrackright}{\isacharcomma}\ auto{\isacharparenright}\isanewline
\ \ \ \ \isacommand{hence}\isamarkupfalse%
\ {\isachardoublequoteopen}{\isacharparenleft}{\isasymlambda}n{\isachardot}\ {\isasymphi}\ n\ t{\isacharparenright}\ {\isacharminus}{\isacharminus}{\isacharminus}{\isacharminus}{\isachargreater}\ complex{\isacharunderscore}of{\isacharunderscore}real\ {\isacharparenleft}exp\ {\isacharparenleft}{\isacharminus}{\isacharparenleft}t{\isacharcircum}{\isadigit{2}}{\isacharparenright}\ {\isacharslash}\ {\isadigit{2}}{\isacharparenright}{\isacharparenright}{\isachardoublequoteclose}\isanewline
\ \ \ \ \ \ \isacommand{apply}\isamarkupfalse%
\ {\isacharparenleft}rule\ Lim{\isacharunderscore}transform{\isacharparenright}\isanewline
\ \ \ \ \ \ \isacommand{by}\isamarkupfalse%
\ {\isacharparenleft}rule\ Lim{\isacharunderscore}null{\isacharunderscore}comparison\ {\isacharbrackleft}OF\ main\ main{\isadigit{2}}{\isacharbrackright}{\isacharparenright}\isanewline
\ \ \ \ \isacommand{thus}\isamarkupfalse%
\ {\isachardoublequoteopen}{\isacharparenleft}{\isasymlambda}n{\isachardot}\ {\isasymphi}\ n\ t{\isacharparenright}\ {\isacharminus}{\isacharminus}{\isacharminus}{\isacharminus}{\isachargreater}\ char\ std{\isacharunderscore}normal{\isacharunderscore}distribution\ t{\isachardoublequoteclose}\isanewline
\ \ \ \ \ \ \isacommand{by}\isamarkupfalse%
\ {\isacharparenleft}subst\ char{\isacharunderscore}std{\isacharunderscore}normal{\isacharunderscore}distribution{\isacharparenright}\isanewline
\ \ \isacommand{qed}\isamarkupfalse%
\isanewline
\ \ \isacommand{thus}\isamarkupfalse%
\ {\isacharquery}thesis\isanewline
\ \ \ \ \isacommand{apply}\isamarkupfalse%
\ {\isacharparenleft}intro\ levy{\isacharunderscore}continuity{\isacharparenright}\isanewline
\ \ \ \ \isacommand{apply}\isamarkupfalse%
\ {\isacharparenleft}rule\ real{\isacharunderscore}distribution{\isacharunderscore}distr\ {\isacharbrackleft}OF\ S{\isacharunderscore}rv{\isacharbrackright}{\isacharparenright}\isanewline
\ \ \ \ \isacommand{unfolding}\isamarkupfalse%
\ real{\isacharunderscore}distribution{\isacharunderscore}def\ real{\isacharunderscore}distribution{\isacharunderscore}axioms{\isacharunderscore}def\isanewline
\ \ \ \ \isacommand{apply}\isamarkupfalse%
\ {\isacharparenleft}simp\ add{\isacharcolon}\ prob{\isacharunderscore}space{\isacharunderscore}normal{\isacharunderscore}density{\isacharparenright}\isanewline
\ \ \ \ \isacommand{unfolding}\isamarkupfalse%
\ {\isasymphi}{\isacharunderscore}def\ S{\isacharprime}{\isacharunderscore}def\ \isacommand{by}\isamarkupfalse%
\ {\isacharminus}\isanewline
\isacommand{qed}
\end{isabellebody}
}


\end{document}
